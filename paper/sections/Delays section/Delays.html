
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Learning Delays &#8212; SNN Sound Localization</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/style-mods.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://comob-project.github.io/snn-sound-localization/paper/sections/Delays section/Delays.html" />
    <link rel="shortcut icon" href="../../../_static/headphone-logo.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">SNN Sound Localization</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../index.html">
   About
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../Contributing.html">
   How to contribute
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/comob-project/snn-sound-localization/discussions/categories/q-a">
   Discussion forum
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Background.html">
   Background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Questions.html">
   Questions &amp; challenges
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Starting-Notebook.html">
   Starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Workshop_1_Write_Up.html">
   Workshop 1 Write-up
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Solving_problem_with_delay_learning.html">
   Vanilla sound localization problem with a single delay layer (non-spiking)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/SNN_sound_W1W2_threshold_plot.html">
   Modified from starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Quick_Start_250HzClassification_CleanVersion.html">
   Quick Start Notebook with 250 Hz input
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Quick_Start_250HzClassification.html">
   Quick Start Notebook with 250 Hz input
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Quick_Start.html">
   Quick Start Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Optimizing-Membrane-Time-Constant.html">
   Improving Performance: Optimizing the membrane time constant
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Learning_delays_major_edit2.html">
   Learning delays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Noise_robustness.html">
   Robustness to Noise and Dropout
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/IE-neuron-distribution.html">
   Analysing Dale’s law and distribution of excitatory and inhibitory neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Excitatory-only-localisation.html">
   Sound localisation with excitatory-only inputs surrogate gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Dynamic_threshold.html">
   Dynamic threshold
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Dales_law.html">
   Sound localisation following Dale’ law
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Dales_Law_Follow_Up.html">
   Analysing Dale’s law and distribution of excitatory and inhibitory neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Analysing-Trained-Networks.html">
   (WIP) Analysing trained networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Compute%20hessians%20%28jax%20version%29.html">
   Compute hessians (jax version)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Analysing-Trained-Networks-Part2.html">
   Analysing trained networks - workshop edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Altering_output_neurons.html">
   Altering Output Neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../research/Alt-Filter-and-Fire_Neuron_Model_SNN.html">
   Filter-and-Fire Neuron Model
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../_sources/paper/sections/Delays section/Delays.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/comob-project/snn-sound-localization"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/comob-project/snn-sound-localization/edit/main/paper/sections/Delays section/Delays.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Learning Delays
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methods">
   Methods
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-and-discussion">
   Results and discussion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Learning Delays</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Learning Delays
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#methods">
   Methods
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results-and-discussion">
   Results and discussion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="learning-delays">
<h1>Learning Delays<a class="headerlink" href="#learning-delays" title="Permalink to this headline">¶</a></h1>
<p>This section introduces a simple method to solve the sound localization problem with only learnable delays</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Studying the computational properties of axonal transmissions goes as far back as in \citealp{LAJ1948}. In this study, it was shown that with the right network setup, axonal delays can be utilized to transform a temporal cue to a spatial one for sound localization. This study was a leading study in terms of using delays explicitly to explain a neuronal function. It paved the way for others to follow, like the study by \citealp{KLWH2001}, where they investigated the question of how ITD computation maps can arise ontogenetically in the laminar nucleus of the barn owl. They showed that interaural time differences (ITD) computational maps emerge from the combined effect of a Hebbian spike-based learning rule and its transmission along the presynaptic axon. Thus, from this study, another role of axonal delays can be inferred. They shape network structure when coupled with temporal learning rules. Based on this insight, several studies investigated the combined effect of spike timing-dependent plasticity (STDP), axonal conduction delays and oscillatory activity on recurrent connections in spiking networks. \citealp{KBTG2013} demonstrated the selective potentiation of recurrent connections when the beforementioned computational considerations are taken into account. Also, \citealp{HKTI2016} showed that neural selection for memory formation depends on neural competition. In turn, for neural competition to emerge in recurrent networks, spontaneously induced neural oscillation coupled with STDP and axonal delays are a perquisite.</p>
<p>Coupling conduction delays with STDP seems like a reasonable choice. The sign of the STDP rule depends on the order of post- and pre-synpatic spiking, which axonal delays can effectively reverse. For example, if the presynaptic spikes arrive at the synapse before the backpropagated action potential this would lead a synpatic depression. However, reducing the axonal transmission speed would lead to potentiation. In this line of thought, \citealp{MAVT2017} studied the combined role of delays and STDP on the emergent synaptic structure in neural networks. It was shown that, qualitatively different connectivity patterns arise due to the interplay between axonal and dendritic delays, as the synapse and cell body can have different temporal spike order.</p>
<p>Aside from their role in modeling cortical functions or shaping a network’s synaptic structure, another line of research emerged from the seminal work by \citealp{EMI2006}. They showed that when including conduction delays and spike-timing dependent plasticity (STDP) into their simulation of realistic neural models, polychronous groups of neurons emerge. These groups show time-locked spiking pattern with millisecond precision. Subsequent studies investigated the properties and functions of such neuronal groups. For example, \citealp{BSEI2010} demonstrated the natural emergence of large memory content and working memory when the neuronal model exploits temporal codes. Specifically, short term plasticity can briefly strengthen the synapses of specific polychronous neuronal groups (PNG) resulting in an enchantment in their spontaneous reactivation rates.  In a qualitatively different study, \citealp{EIAS2018} showed that networks that exhibit PNG possess potential capabilities that might solve the dynamic binding problem. These networks respond with stable saptio-temporal spike trains when presented with input images in the form of randomized Poisson spike trains. The functionality of these kind of networks emerged due to the interplay of various factors including: i) random distribution of axonal delays ii) STDP ii) lateral, bottom-up and top-down synaptic connections.</p>
<p>Finally, it should be noted that most of the studies that incorporate axonal and/or dendritic delays, include them as a non-learnable parameter. Few studies investigated the possibility of training transmission delays in order to enhance the computational capabilities of spiking neural networks (SNN). \citealp{TM2017} proposed an algorithm that modifies the axonal delays and synaptic efficacy in both supervised and unsupervised approaches.  The learning method used approximates the Expectation-Maximization (EM) algorithm and after training, the network learns to  map spatio-temporal input-output spike patterns. Thus, EM is one way to train SNN that are cast as probabilistic models. Another approach that exploits the massive infrastructure that is laid out the deep learning literature is the work by \citealp{HHM2023}. In this work, delays are represented as 1D convolutions through time, where the kernels include a single per-synapse non-zero weight. The temporal position of these non-zero weights corresponds to the desired delays. The proposed method co-trains weights and delays and is based on the Dilated Convolution
with Learnable Spacings (DCLS) algorithm [\citealp{ITT2023}].</p>
<p>In this work we propose a delay learning algorithm that is simple and efficient. The delay learning is mediated by a differentiable delay layer (DDL). This layer can be inserted between any two layers in an SNN in order to learn the appropriate delay to solve a machine learning task. This DDL is architecture agnostic. Also, the delays can be learned independently of weights.</p>
</div>
</div>
<div class="section" id="methods">
<h1>Methods<a class="headerlink" href="#methods" title="Permalink to this headline">¶</a></h1>
<p>The DDL is, mainly, based on a 1D version of the spatial transformer (STN) network \citealp{JSZK2015}. The STN is a differentiable module that can be added into conventional neural networks (CNNs) architectures to empower them with the ability to spatially transform feature maps in a differentiable way. This addition leads to CNNs models that are invariant to various spatial transformations like translation, scaling and rotation. Image manipulations are inherently  not differentiable, because pixels are a discrete. However, this problem is overcome by the application of an interpolation  (for example bi-linear) after the spatial transformation.</p>
<p>The DDL is a 1D version of the spatial transformer where the only transformation done is translation. Translation of a spike along the time dimension can be thought of as a translation of a pixel along the spatial coordinates. The general affine transformation matrix for the 2D case takes the form in the following euqation:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 
	sr_1 &amp; sr_2 &amp; t_x\\
	sr_3 &amp; sr_4 &amp; t_y\\
	0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix} 
    x_t\\
	y_t\\
	1
\end{bmatrix} = \begin{bmatrix} 
    x_s\\
	y_s\\
	1
\end{bmatrix}
\end{split}\]</div>
<p>In the above equation, <span class="math notranslate nohighlight">\({sr_1,~sr_2,~sr_3,~sr_4}\)</span> are the elements responsible for the linear transformations of scaling and rotation. <span class="math notranslate nohighlight">\({t_x~and~t_y}\)</span> are the translations in the x-axis and y-axis respectively. <span class="math notranslate nohighlight">\({x_t~and~y_t}\)</span> are the location of a spike/pixel (in case of spikes y = 0) in the target/output grid, while <span class="math notranslate nohighlight">\({x_s~and~y_s}\)</span> are the location of the source grid. A grid can be an image or a 2D array of spike trains. For the case of only translation along the x-axis, the affine transformation matrix becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 
	1 &amp; 0 &amp; t_x\\
	0 &amp; 1 &amp; 0\\
	0 &amp; 0 &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>Conventionally, for the spatial transformer, after the projection of the target grid onto the source grid comes the process of interpolation as the transformed pixel might not coincide with a representative one in the source grid/image. Hence, interpolation is performed to estimate the value of the transformed pixel from the surrounding ones. However, applying this process to spike trains can lead to a distortion of the spikes as the allowed values are only 1s and 0s. To avoid this, the translation element <span class="math notranslate nohighlight">\({t_x}\)</span> should be multiples of the minimum delay. Thus, any transformed spike location from the target grid will find a matching spike location in the source grid. Interpolation (for example bi-linear) will pick the coincident source spike with a weighting of one and provide zero weighting for any other nearby spikes.  This process is summarized visually in Fig. 1. The input spike trains to the DDL should be padded by zeros so as to not lose information after translation.
<br>
<br></p>
<p align="center">
<img src="DDL.png" alt="DDL" width="600"/>
</p>
<center>Figure 1:  Structure of the DDL. The DDL shifts an input spike train by applying translation then interpolation.</center>
<br>	 
<br>
Only the DDL is needed to solve the sound localization problem, where the output classes are the target IPD. As shown in Fig. 2, the DDL inserted between the input and output nodes is sufficient to solve the sound localization problem. In this cases, the weights are set to one and the biases to zero.
<br>
<br>
<p align="center">
<img src="Network.png" alt="Network" width="600"/>
</p>
<p>Figure 2: The model architecture. The DLL inserted between the input and out nodes is sufficient to solve the sound localization problem. The output nodes are IPD classes spanning the range <span class="math notranslate nohighlight">\({[-90^o,~85^o]~in~5^o}\)</span> steps.</p>
<br>	 
<br>
Though the DLL shifts the spike trains in a differentiable way, this shift has a meaningful impact through a multiplicative synapse. Also, to facilitate learning, a non-spiking output was utilized. Thus, these two points taken together, the voltage at the output takes the form:
<div class="math notranslate nohighlight">
\[\begin{split}
v_i = -\sum_{t=0}^{T}(u_{1i}(t) - u_{2i}(t))^2 \\
\tau_m*\frac{du_{xi}}{dt} = -u_{xi} + S_x(t)  \;\;\;\;  x \in \{1,2\} \\
S_x(t) = \sum_{z=1}^{n} \delta(t-t_x^z)
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\({v_i}\)</span> is the voltage at output node <span class="math notranslate nohighlight">\({i.~u_{xi}}\)</span> is the dendritic potential. <span class="math notranslate nohighlight">\(\tau_m\)</span> is the time constant of a dendritic branch. <span class="math notranslate nohighlight">\(S_x(t)\)</span> is the input spike train. The form <span class="math notranslate nohighlight">\((u_{1i}(t) - u_{2i}(t))^2\)</span> looks like a squared error and might, at first glance, seem not biological, but the expanded term (as seen from the following equation) is a from of multi-synaptic multiplicative-additive interaction.</p>
<div class="math notranslate nohighlight">
\[
(u_{1i}(t) - u_{2i}(t))^2 = u_{1i}(t)^2 -2*u_{1i}(t)*u_{2i}(t) + u_{2i}(t)^2
\]</div>
</div>
<div class="section" id="results-and-discussion">
<h1>Results and discussion<a class="headerlink" href="#results-and-discussion" title="Permalink to this headline">¶</a></h1>
<p>In this section, the problem complexity is increased up to 36 output units spanning an IPD range of <span class="math notranslate nohighlight">\({[-90,~85]}\)</span> with a step of <span class="math notranslate nohighlight">\({5^o}\)</span>. Employing the DDL to solve such a task leads to the spike raster plots shown in Fig. 3.
<br>
<br></p>
<p align="center">
<img src="Results 1.png" alt="Results 1" width="1200"/>
</p>
<center>Figure 3: Spike histograms for IPDs before and after training.</center>
<br>	 
<br>
To facilitate the search for a solution while using the DDL, we assume that the delay lines coming from one ear is fixed, while the delay lines from the other ear is adaptable starting from an initial default value. Thus, the learned delays can be thought of as the change in delays, which is added to the default delays. This approach leads to the results shown in Fig. 3b, where the top of the spike histograms is fixed, while the bottom half has a graded shift in its spike plots. Such results were achieved with the application of time constant decay that leads to a decrease in loss as shown in Fig. 4a. 
<br>
<br>
<p align="center">
<img src="Loss and dist.png" alt="Loss and dist" width="1000"/>
</p>
<center>Figure 4: Performance metrics one. A) Loss as a function of the epochs. B) The difference between the True and predicted IPDs in a test batch.</center>
<br>	 
<br>
Further analysis of such solutions warrants testing them in different forms. In this regards, we start by displaying the distribution of the errors between the true IPDs and predicted IPDs in a test batch as shown in Fig. 4b, which shows an almost log-normal distribution. In addition, we show the confusion matrix for both the training and testing batches on the right images of Fig. 5, and the difference between the true and predicted IPDs on the left images of Fig. 5.
<br>
<br>
<p align="center">
<img src="Confuse.png" alt="Confuse" width="1000"/>
</p>
<center>Figure 5: Performance metrics two. Here is shown the distribution and confusion matrix between true IPD values and A) training batch IPD estimates, B) testing batch IPD estimates.</center>
<br>	 
<br></div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./paper/sections/Delays section"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By COMOB, the project for collaborative modelling of the brain<br/>
    
      <div class="extra_footer">
        <small>
  Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA</a>:
  You may use this work, with attribution, in other freely available works.
</small>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>