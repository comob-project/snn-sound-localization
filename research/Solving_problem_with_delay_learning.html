
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vanilla sound localization problem with a single delay layer (non-spiking) &#8212; SNN Sound Localization</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style-mods.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://comob-project.github.io/snn-sound-localization/research/Solving_problem_with_delay_learning.html" />
    <link rel="shortcut icon" href="../_static/headphone-logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Modified from starting Notebook" href="SNN_sound_W1W2_threshold_plot.html" />
    <link rel="prev" title="Workshop 1 Write-up" href="Workshop_1_Write_Up.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">SNN Sound Localization</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   About
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Contributing.html">
   How to contribute
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/comob-project/snn-sound-localization/discussions/categories/q-a">
   Discussion forum
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Background.html">
   Background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Questions.html">
   Questions &amp; challenges
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Starting-Notebook.html">
   Starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Workshop_1_Write_Up.html">
   Workshop 1 Write-up
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Vanilla sound localization problem with a single delay layer (non-spiking)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SNN_sound_W1W2_threshold_plot.html">
   Modified from starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Quick_Start_250HzClassification.html">
   Quick Start Notebook with 250 Hz input
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Optimizing-Membrane-Time-Constant.html">
   Improving Performance: Optimizing the membrane time constant
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Quick_Start.html">
   Quick Start Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Noise_robustness.html">
   Robustness to Noise and Dropout
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Learning_delays_major_edit2.html">
   Learning delays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="IE-neuron-distribution.html">
   Analysing Dale’s law and distribution of excitatory and inhibitory neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Filter-and-Fire_Neuron_Model_SNN.html">
   Filter-and-Fire Neuron Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dynamic_threshold.html">
   Dynamic threshold
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Excitatory-only-localisation.html">
   Sound localisation with excitatory-only inputs surrogate gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dales_Law_Follow_Up.html">
   Analysing Dale’s law and distribution of excitatory and inhibitory neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dales_law.html">
   Sound localisation following Dale’ law
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Compute%20hessians%20%28jax%20version%29.html">
   Compute hessians (jax version)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Analysing-Trained-Networks.html">
   (WIP) Analysing trained networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Analysing-Trained-Networks-Part2.html">
   Analysing trained networks - workshop edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Altering_output_neurons.html">
   Altering Output Neurons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Alt-Filter-and-Fire_Neuron_Model_SNN.html">
   Filter-and-Fire Neuron Model
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/research/Solving_problem_with_delay_learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/comob-project/snn-sound-localization"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/comob-project/snn-sound-localization/edit/main/research/Solving_problem_with_delay_learning.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/comob-project/snn-sound-localization/main?urlpath=tree/research/Solving_problem_with_delay_learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/comob-project/snn-sound-localization/blob/main/research/Solving_problem_with_delay_learning.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#input-target-generator">
   Input-target generator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#batch-generator-and-input-visualization">
   Batch generator and input visualization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#delay-layer">
   Delay Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#surrogate-delays-update">
   Surrogate Delays update
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synaptic-integration-function">
   Synaptic Integration function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-loop">
   Training loop
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-measure-one">
   Performance measure one
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-measure-two">
   Performance measure two
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Vanilla sound localization problem with a single delay layer (non-spiking)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#definitions">
   Definitions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#input-target-generator">
   Input-target generator
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#batch-generator-and-input-visualization">
   Batch generator and input visualization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#delay-layer">
   Delay Layer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#surrogate-delays-update">
   Surrogate Delays update
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#synaptic-integration-function">
   Synaptic Integration function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-loop">
   Training loop
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-measure-one">
   Performance measure one
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#performance-measure-two">
   Performance measure two
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="vanilla-sound-localization-problem-with-a-single-delay-layer-non-spiking">
<h1>Vanilla sound localization problem with a single delay layer (non-spiking)<a class="headerlink" href="#vanilla-sound-localization-problem-with-a-single-delay-layer-non-spiking" title="Permalink to this headline">¶</a></h1>
<p>Here, I showcase the solution to the sound localization problem using only differentiable delays. For this project, this is the fruit of the work done on differentiable delays.
I truly grateful for everyone that I interacted with in this project. For me, it was a nice experience and I hope we can do similar projectes to tackle different projects in the future.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Main docstring</span>
<span class="sd">&quot;&quot;&quot;Solving the sound localization problem with only differentiable delays (non-spiking)</span>

<span class="sd">Functions:</span>
<span class="sd">    input_signal: outputs poisson generated spike trains for a given input IPD</span>
<span class="sd">    get_batch: generate a fixed size patch of input-targets from the input-signal function</span>
<span class="sd">    snn_sl: defines the synaptic integration function</span>
<span class="sd">    analyse: a visualization function for the results of the training</span>

<span class="sd">Classes:</span>
<span class="sd">    Delaylayer: defines the delaylayer object</span>
<span class="sd">    Delayupdate: defines the object responisble for the application of surrogate delay updates</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;Solving the sound localization problem with only differentiable delays (non-spiking)\n\nFunctions:\n    input_signal: outputs poisson generated spike trains for a given input IPD\n    get_batch: generate a fixed size patch of input-targets from the input-signal function\n    snn_sl: defines the synaptic integration function\n    analyse: a visualization function for the results of the training\n\nClasses:\n    Delaylayer: defines the delaylayer object\n    Delayupdate: defines the object responisble for the application of surrogate delay updates\n&#39;
</pre></div>
</div>
</div>
</div>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sn</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">sci_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;torch._C.Generator at 0x7fab02a93330&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="definitions">
<h2>Definitions<a class="headerlink" href="#definitions" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Relevant definitions</span>
<span class="c1"># Not using Brian so we just use these constants to make equations look nicer below</span>
<span class="n">SECOND</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MS</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">HZ</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># Stimulus and simulation parameters</span>
<span class="n">DT</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">MS</span>            <span class="c1"># Large time step to make simulations run faster for tutorial</span>
<span class="n">ENVELOPE_POWER</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># Higher values make sharper envelopes, easier</span>
<span class="n">RATE_MAX</span> <span class="o">=</span> <span class="mi">600</span> <span class="o">*</span> <span class="n">HZ</span>   <span class="c1"># Maximum Poisson firing rate</span>
<span class="n">F</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="n">HZ</span>            <span class="c1"># Stimulus frequency</span>
<span class="n">DURATION</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">*</span> <span class="n">MS</span>  <span class="c1"># Stimulus duration</span>
<span class="n">DURATION_STEPS</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">DURATION</span> <span class="o">/</span> <span class="n">DT</span><span class="p">))</span> <span class="c1"># The length of the stimulus</span>
<span class="n">ANG_STEP</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># Minimum angle difference between the IPD classes</span>
<span class="n">NUMBER_CLASSES</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">180</span><span class="o">/</span><span class="n">ANG_STEP</span><span class="p">)</span> <span class="c1"># Number of IPD classes</span>
<span class="c1"># Training parameters</span>
<span class="n">NB_EPOCHS</span> <span class="o">=</span> <span class="mi">20000</span>  
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="sd">&quot;&quot;&quot;Delay paramters and functions&quot;&quot;&quot;</span>
<span class="n">MAX_DELAY</span> <span class="o">=</span> <span class="mi">20</span>  <span class="c1"># Assumed to be in ms</span>
<span class="n">NUMBER_INPUTS</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># Number of input spikes trains corresponding to the two ears</span>
<span class="n">EFFECTIVE_DURATION</span> <span class="o">=</span> <span class="n">MAX_DELAY</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">DURATION</span> <span class="o">/</span> <span class="n">DT</span><span class="p">))</span>
<span class="n">TAU</span><span class="p">,</span> <span class="n">TAU_DECAY</span><span class="p">,</span> <span class="n">TAU_MINI</span><span class="p">,</span> <span class="n">TAU_DECAY_FLAG</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="kc">True</span>  <span class="c1"># Time constant decay settings</span>
<span class="n">ROUND_DECIMALS</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># For the stability of the delay layer</span>
<span class="n">FIX_FIRST_INPUT</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Fix the first input in delay learning</span>
<span class="n">SHOW_IMAGE</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Visualization of the whole raster plot or target spikes</span>
<span class="n">SYNAPSE_TYPE</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># 0 for multiplicaitve, 1 for subtractive</span>
<span class="n">LR_DELAY</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># Learning rate for the differentiable delays</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="input-target-generator">
<h2>Input-target generator<a class="headerlink" href="#input-target-generator" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Input target generators</span>
<span class="k">def</span> <span class="nf">input_signal</span><span class="p">(</span><span class="n">ipd_choice</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates input-target pairs</span>

<span class="sd">    Parameters:</span>
<span class="sd">        ipd_choice(int): input ipd in degrees</span>

<span class="sd">    Returns:</span>
<span class="sd">        spikes_out(numpy.ndarray, (NUMBER_CLASSES, NUMBER_INPUTS, EFFECTIVE_DURATION)): The input spike trains</span>
<span class="sd">        ipds_hot(numpy.ndarray, (NUMBER_CLASSES,)): One hot incoding of the classes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ipds_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUMBER_CLASSES</span><span class="p">,))</span>
    <span class="n">ipds_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">ANG_STEP</span><span class="p">)</span>
    <span class="n">ipds_hot</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">ipds_norm</span><span class="o">==</span><span class="n">ipd_choice</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ipds_rad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ipd_choice</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">180</span><span class="p">]</span><span class="o">*</span><span class="n">NUMBER_CLASSES</span><span class="p">)</span>
    <span class="n">time_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">DURATION_STEPS</span><span class="p">)</span> <span class="o">*</span> <span class="n">DT</span>  <span class="c1"># array of times</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">F</span> <span class="o">*</span> <span class="n">time_axis</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span>  <span class="c1"># array of phases corresponding to those times with random offset</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">NUMBER_INPUTS</span><span class="p">,</span> <span class="n">DURATION_STEPS</span><span class="p">))</span>
    <span class="n">zeros_pad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">NUMBER_INPUTS</span><span class="p">,</span> <span class="n">MAX_DELAY</span><span class="p">))</span>
    <span class="n">theta</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">theta</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">ipds_rad</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
    <span class="c1"># now generate Poisson spikes at the given firing rate as in the previous notebook</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">NUMBER_INPUTS</span><span class="p">,</span> <span class="n">DURATION_STEPS</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">RATE_MAX</span> <span class="o">*</span> \
        <span class="n">DT</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)))</span> <span class="o">**</span> <span class="n">ENVELOPE_POWER</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">zeros_pad</span><span class="p">,</span> <span class="n">zeros_pad</span><span class="p">,</span> <span class="n">spikes_out</span><span class="p">,</span> <span class="n">zeros_pad</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">spikes_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">spikes_out</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">yield</span> <span class="p">(</span><span class="n">spikes_out</span><span class="p">,</span> <span class="n">ipds_hot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="batch-generator-and-input-visualization">
<h2>Batch generator and input visualization<a class="headerlink" href="#batch-generator-and-input-visualization" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Batch generator function and visualization of inputs</span>
<span class="k">def</span> <span class="nf">get_batch</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generates a batch of input-target pairs with a predefined length</span>

<span class="sd">    Parameters:</span>

<span class="sd">    Returns:</span>
<span class="sd">        inputs(torch.Tensor, (BATCH_SIZE, NUMBER_CLASSES, NUMBER_INPUTS, EFFECTIVE_DURATION)): A batch of input spike trains</span>
<span class="sd">        targets(torch.Tensor, (BATCH_SIZE, NUMBER_CLASSES)): A batch of one hot incoded targets</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">choice</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">ANG_STEP</span><span class="p">))</span>
        <span class="n">value_input</span><span class="p">,</span> <span class="n">value_target</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">input_signal</span><span class="p">(</span><span class="n">ipd_choice</span><span class="o">=</span><span class="n">choice</span><span class="p">))</span>
        <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value_input</span><span class="p">)</span>
        <span class="n">targets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value_target</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">inputs</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>

<span class="c1"># The below code is for the visualization of the input spike trains</span>
<span class="n">spikes_out_all</span><span class="p">,</span> <span class="n">ipds_hot_all</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">ANG_STEP</span><span class="p">):</span>
    <span class="n">spikes_out_temp</span><span class="p">,</span> <span class="n">ipds_hot_temp</span><span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">input_signal</span><span class="p">(</span><span class="n">ipd_choice</span><span class="o">=</span><span class="n">i</span><span class="p">))</span>
    <span class="n">spikes_out_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spikes_out_temp</span><span class="p">)</span>
    <span class="n">ipds_hot_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ipds_hot_temp</span><span class="p">)</span>
<span class="n">spikes_out_all</span><span class="p">,</span> <span class="n">ipds_hot_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">spikes_out_all</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ipds_hot_all</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ipds_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">ANG_STEP</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">SHOW_IMAGE</span><span class="p">:</span>
        <span class="n">image_1</span> <span class="o">=</span> <span class="n">spikes_out_all</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">image_1</span><span class="p">[</span><span class="n">image_1</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.75</span>
        <span class="n">image_2</span> <span class="o">=</span> <span class="n">spikes_out_all</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">*</span><span class="mf">0.5</span>
        <span class="n">image_2</span><span class="p">[</span><span class="n">image_2</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.75</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">)),</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                  <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;seismic&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">spikes_out_all</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;IPD = </span><span class="si">{</span><span class="n">ipds_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">12</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Input neuron index&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">6</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Classes&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Solving_problem_with_delay_learning_10_0.png" src="../_images/Solving_problem_with_delay_learning_10_0.png" />
</div>
</div>
</div>
<div class="section" id="delay-layer">
<h2>Delay Layer<a class="headerlink" href="#delay-layer" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Delay layer</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DelayLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The delay layer class</span>

<span class="sd">    This class defines an array of differentiable delays of size (NUMBER_INPUTS, NUMBER_CLASSES) </span>
<span class="sd">    to applied between any SNN layers</span>

<span class="sd">    Attributes:</span>
<span class="sd">        self.max_delay(int): maximum value of the applied delays</span>
<span class="sd">        self.trainable_delays(boolen): A flag that defines whether the delay array is differentiable or not</span>
<span class="sd">        self.number_inputs(int): the number of the input spike trains</span>
<span class="sd">        self.constant_delays(boolen): the initialized delays all have a constant value</span>
<span class="sd">        self.constant_value(int): the initialized delays constant value</span>
<span class="sd">        self.lr_delay(float): learning rate for the differentiable delays</span>
<span class="sd">        self.effective_duration(int): length of the agumented input duration in ms</span>
<span class="sd">        self.delays_out(int, (NUMBER_INPUTS, NUMBER_CLASSES)): the initialized delay array</span>
<span class="sd">        self.optimizer_delay(torch.optim): the backprop optimizer for the differentiable delays</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_delay_in</span><span class="o">=</span><span class="mi">19</span><span class="p">,</span> <span class="n">train_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_ear</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">constant_delays</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">constant_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lr_delay</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_delay</span> <span class="o">=</span> <span class="n">max_delay_in</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainable_delays</span> <span class="o">=</span> <span class="n">train_delays</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">number_inputs</span> <span class="o">=</span> <span class="n">NUMBER_INPUTS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constant_delays</span> <span class="o">=</span> <span class="n">constant_delays</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">constant_value</span> <span class="o">=</span> <span class="n">constant_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_delay</span> <span class="o">=</span> <span class="n">lr_delay</span>  <span class="c1"># Not fine-tuned much</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">effective_duration</span> <span class="o">=</span> <span class="n">EFFECTIVE_DURATION</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delays_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_delay_vector</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_delay</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_optimizer</span><span class="p">()</span>

    <span class="c1"># Delays with constant or random initialisation</span>
    <span class="c1"># Might think of other ways to initialize delays and their effect on performance</span>
    <span class="k">def</span> <span class="nf">_init_delay_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;the applied delays initializer</span>

<span class="sd">        Parameters:</span>

<span class="sd">        Returns:</span>
<span class="sd">            delays(int, (NUMBER_INPUTS, NUMBER_CLASSES)): the initialized delay array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">FIX_FIRST_INPUT</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">number_inputs</span> <span class="o">=</span> <span class="n">NUMBER_INPUTS</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">number_inputs</span> <span class="o">=</span> <span class="n">NUMBER_INPUTS</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">constant_delays</span><span class="p">:</span>
            <span class="n">delays</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">constant_value</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">number_inputs</span><span class="p">,</span> <span class="n">NUMBER_CLASSES</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_delays</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">delays_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_delay</span><span class="p">,</span>
                                             <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">number_inputs</span><span class="p">,</span> <span class="n">NUMBER_CLASSES</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
            <span class="c1"># delays_numpy = np.arange(0, 20, 1)</span>
            <span class="n">delays</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parameter</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">delays_numpy</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable_delays</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">delays</span>

    <span class="k">def</span> <span class="nf">_init_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;the delay optimizer initializer</span>

<span class="sd">        Parameters:</span>

<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optimizer_delay</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">delays_out</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_delay</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer_delay</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spikes_in</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward pass through the delay layer</span>

<span class="sd">        Parameters:</span>
<span class="sd">            spikes_in(torch.Tensor, (BATCH_SIZE, NUMBER_INPUTS, NUMBER_CLASSES, EFFECTIVE_DURATION)): the input spike trains to be shifted</span>

<span class="sd">        Returns:</span>
<span class="sd">            output_train(torch.Tensor, (BATCH_SIZE, NUMBER_INPUTS, NUMBER_CLASSES, EFFECTIVE_DURATION)): the shifted(delay applied) spike trains</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_train</span> <span class="o">=</span> <span class="n">spikes_in</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">FIX_FIRST_INPUT</span><span class="p">:</span>
            <span class="n">input_first</span> <span class="o">=</span> <span class="n">input_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="n">input_train</span> <span class="o">=</span> <span class="n">input_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">dlys</span> <span class="o">=</span> <span class="n">delay_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">delays_out</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">duration</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_train</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="c1"># initialize M to identity transform and resize</span>
        <span class="n">translate_mat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
        <span class="n">translate_mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">translate_mat</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
        <span class="c1"># translate with delays</span>
        <span class="n">translate_mat</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">duration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dlys</span>
        <span class="c1"># create normalized 1D grid and resize</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">duration</span><span class="p">)</span>
        <span class="n">y_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">duration</span><span class="p">))</span>  <span class="c1"># 1D: all y points are zeros</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">x_t</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x_t</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y_t</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ones</span><span class="p">])</span>   <span class="c1"># an array of points (x, y, 1) shape (3, :)</span>
        <span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">duration</span><span class="p">)))</span>
        <span class="c1"># transform the sampling grid i.e. batch multiply</span>
        <span class="n">translate_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">translate_mat</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
        <span class="c1"># reshape to (num_batch, height, width, 2)</span>
        <span class="n">translate_grid</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">translate_grid</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">x_points</span> <span class="o">=</span> <span class="n">translate_grid</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">corr_center</span> <span class="o">=</span> <span class="p">((</span><span class="n">x_points</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">duration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="c1"># grab 4 nearest corner points for each (x_t, y_t)</span>
        <span class="n">corr_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">corr_center</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="n">ROUND_DECIMALS</span><span class="p">))</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">corr_right</span> <span class="o">=</span> <span class="n">corr_left</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="c1"># Calculate weights</span>
        <span class="n">weight_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">corr_right</span> <span class="o">-</span> <span class="n">corr_center</span><span class="p">)</span>
        <span class="n">weight_left</span> <span class="o">=</span> <span class="p">(</span><span class="n">corr_center</span> <span class="o">-</span> <span class="n">corr_left</span><span class="p">)</span>

        <span class="c1"># Padding for values that are evaluated outside the input range</span>
        <span class="n">pad_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">corr_right</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">duration</span>
        <span class="n">pad_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">amin</span><span class="p">(</span><span class="n">corr_left</span><span class="p">))</span>
        <span class="n">zeros_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">zeros_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">input_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">input_train</span><span class="p">,</span> <span class="n">zeros_right</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
        <span class="c1"># Get the new values after the transformation</span>
        <span class="n">value_left</span> <span class="o">=</span> <span class="n">input_train</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                                 <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">classes</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">corr_left</span><span class="p">][:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">value_right</span> <span class="o">=</span> <span class="n">input_train</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                                  <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">classes</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">corr_right</span><span class="p">][:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="c1"># compute output</span>
        <span class="n">output_train</span> <span class="o">=</span> <span class="n">weight_right</span><span class="o">*</span><span class="n">value_left</span> <span class="o">+</span> <span class="n">weight_left</span><span class="o">*</span><span class="n">value_right</span>
        <span class="k">if</span> <span class="n">FIX_FIRST_INPUT</span><span class="p">:</span>
            <span class="n">output_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">input_first</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">output_train</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_train</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="surrogate-delays-update">
<h2>Surrogate Delays update<a class="headerlink" href="#surrogate-delays-update" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Surrogate Delays</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DelayUpdate</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The delay update rounder and clamper class</span>

<span class="sd">    This class defines an object through which we can guarantee that the applied delays are whole numbers</span>
<span class="sd">    with in a specified range</span>

<span class="sd">    Attributes:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">delays</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;the forward pass through this class</span>

<span class="sd">        Parameters:</span>

<span class="sd">        Returns:</span>
<span class="sd">            delays_forward(int, (NUMBER_INPUTS, NUMBER_CLASSES)): the applied delays to the spike trains after rounding and clamping</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">delays_forward</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">delays</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="n">delay_layer</span><span class="o">.</span><span class="n">max_delay</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">delay_layer</span><span class="o">.</span><span class="n">max_delay</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">delays_forward</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;the backprop pass through this class</span>

<span class="sd">        Parameters:</span>

<span class="sd">        Returns:</span>
<span class="sd">            delays_in(float, (NUMBER_INPUTS, NUMBER_CLASSES)): the acquired delays after the gradient update</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">delays_in</span> <span class="o">=</span> <span class="n">grad_output</span>
        <span class="k">return</span> <span class="n">delays_in</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="synaptic-integration-function">
<h2>Synaptic Integration function<a class="headerlink" href="#synaptic-integration-function" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Synaptic integration function</span>
<span class="c1"># np.random.seed(0)</span>
<span class="c1"># torch.manual_seed(0)</span>

<span class="k">def</span> <span class="nf">snn_sl</span><span class="p">(</span><span class="n">input_spikes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Nonlinear integration of the spike trains to produce a target voltage (soma potential)</span>

<span class="sd">    Parameters:</span>

<span class="sd">    Returns:</span>
<span class="sd">        v_out(torch.Tensor, (BATCH_SIZE, NUMBER_CLASSES)): the result of the nonlinear integration </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_spikes</span> <span class="o">=</span> <span class="n">delay_layer</span><span class="p">(</span><span class="n">input_spikes</span><span class="p">)</span>
    <span class="n">duration_in</span> <span class="o">=</span> <span class="n">delay_layer</span><span class="o">.</span><span class="n">effective_duration</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUMBER_INPUTS</span><span class="p">,</span> <span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">delay_layer</span><span class="o">.</span><span class="n">effective_duration</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">TAU</span><span class="p">)</span>  <span class="c1"># self-decay multiplier</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duration_in</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  <span class="c1"># Simple synaptic kernel application</span>
        <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input_spikes</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">t</span><span class="p">]</span>
    <span class="n">first_mat</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="k">if</span> <span class="n">SYNAPSE_TYPE</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Multiplicative synapse</span>
        <span class="n">v_mul</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">first_mat</span><span class="p">,</span> <span class="n">v</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span>
        <span class="n">v_out</span> <span class="o">=</span> <span class="n">v_mul</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># Subtractive synapse</span>
        <span class="n">v_sub</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">first_mat</span><span class="p">,</span> <span class="n">v</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]))</span>
        <span class="n">v_out</span> <span class="o">=</span> <span class="n">v_sub</span>
    <span class="n">v_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">v_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v_out</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training-loop">
<h2>Training loop<a class="headerlink" href="#training-loop" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Training loop and parameters</span>
<span class="c1"># np.random.seed(0)</span>
<span class="c1"># torch.manual_seed(0)</span>
<span class="c1"># torch.autograd.set_detect_anomaly(True)</span>
<span class="n">delay_layer</span> <span class="o">=</span> <span class="n">DelayLayer</span><span class="p">(</span><span class="n">lr_delay</span><span class="o">=</span><span class="n">LR_DELAY</span><span class="p">,</span> <span class="n">constant_delays</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">constant_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_delay_in</span><span class="o">=</span><span class="n">MAX_DELAY</span><span class="p">)</span>  <span class="c1"># A delay layer object</span>
<span class="n">delay_fn</span> <span class="o">=</span> <span class="n">DelayUpdate</span><span class="o">.</span><span class="n">apply</span> <span class="c1"># An object that medisate the application of surrogate updates to the differentiable delays</span>
<span class="n">optimizer_delay_apply</span> <span class="o">=</span> <span class="n">delay_layer</span><span class="o">.</span><span class="n">optimizer_delay</span> 
<span class="n">log_softmax_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">softmax_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">X_TRAIN</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Y_TRAIN</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NB_EPOCHS</span><span class="p">):</span>
    <span class="n">local_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">y_local</span> <span class="ow">in</span> <span class="n">get_batch</span><span class="p">():</span>
        <span class="n">X_TRAIN</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_local</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">snn_sl</span><span class="p">(</span><span class="n">x_local</span><span class="p">)</span>  <span class="c1"># Apply the synaptic integration with delays</span>
        <span class="n">target</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>  <span class="c1"># Convert the one hot incoded targets to whole numbers for the cross-entropy function</span>
            <span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_local</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">))</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">Y_TRAIN</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="c1"># If use weighting, increase the batch size as sometimes a class is not sampled</span>
        <span class="c1"># loss_w = torch.tensor(1 - (np.unique(target, return_counts=True)[1] / len(target))).float()</span>
        <span class="c1"># loss_fn = nn.NLLLoss(weight = loss_w/loss_w.sum())</span>

        <span class="n">out_prop</span> <span class="o">=</span> <span class="n">log_softmax_fn</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># Apply log-softmax </span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out_prop</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># Apply the cross-entropy loss</span>
        <span class="n">local_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">optimizer_delay_apply</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer_delay_apply</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">TAU_DECAY_FLAG</span><span class="p">:</span>  <span class="c1"># Apply the tau decay operation</span>
        <span class="k">if</span> <span class="n">TAU</span> <span class="o">&gt;=</span> <span class="n">TAU_MINI</span><span class="p">:</span>
            <span class="n">TAU</span> <span class="o">*=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">TAU_DECAY</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">TAU</span> <span class="o">=</span> <span class="n">TAU_MINI</span>
        <span class="c1"># print(&#39;Tau:&#39;, TAU)</span>
    <span class="n">loss_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">local_loss</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">%i</span><span class="s2">: loss=</span><span class="si">%.5f</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">local_loss</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tau: &#39;</span><span class="p">,</span> <span class="n">TAU</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Actual delays clamped: &#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">delay_layer</span><span class="o">.</span><span class="n">delays_out</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
                                                 <span class="nb">min</span><span class="o">=-</span><span class="n">delay_layer</span><span class="o">.</span><span class="n">max_delay</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">delay_layer</span><span class="o">.</span><span class="n">max_delay</span><span class="p">)),</span> <span class="s1">&#39;</span><span class="se">\n\n\n\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">e</span> <span class="o">&gt;=</span> <span class="mi">150</span><span class="p">:</span>  <span class="c1"># Visualization of the spike trains before and after the training</span>
    <span class="c1"># if np.mean(local_loss) &lt; 3.6:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">e</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loss_hist</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs (au)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Mean loss (au)&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n\n\n\n</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="n">trial_input_all</span> <span class="o">=</span> <span class="n">spikes_out_all</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">trial_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">trial_input_all</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="n">trial_out</span> <span class="o">=</span> <span class="n">delay_layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">trial_input</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">fig_1</span><span class="p">,</span> <span class="n">axs_1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs_1</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">SHOW_IMAGE</span><span class="p">:</span>
                <span class="n">image_1</span> <span class="o">=</span> <span class="n">trial_input</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">image_1</span><span class="p">[</span><span class="n">image_1</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.75</span>
                <span class="n">image_2</span> <span class="o">=</span> <span class="n">trial_input</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">*</span><span class="mf">0.5</span>
                <span class="n">image_2</span><span class="p">[</span><span class="n">image_2</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.75</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">)),</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                  <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;seismic&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">trial_input</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;IPD = </span><span class="si">{</span><span class="n">ipds_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">12</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Input neuron index&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">6</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">fig_1</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Before training&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">fig_2</span><span class="p">,</span> <span class="n">axs_2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">trial_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span><span class="n">trial_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">axs_2</span><span class="o">.</span><span class="n">flat</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">SHOW_IMAGE</span><span class="p">:</span>
                <span class="n">image_1</span> <span class="o">=</span> <span class="n">trial_out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">image_1</span><span class="p">[</span><span class="n">image_1</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>
                <span class="n">image_2</span> <span class="o">=</span> <span class="n">trial_out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="n">image_2</span><span class="p">[</span><span class="n">image_2</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span>
                <span class="n">image_2</span><span class="p">[</span><span class="n">image_2</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">image_1</span><span class="p">,</span> <span class="n">image_2</span><span class="p">)),</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;seismic&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">trial_out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;IPD = </span><span class="si">{</span><span class="n">ipds_range</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">30</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">12</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Input neuron index&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">6</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">30</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">fig_2</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;After training&#39;</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s2">&quot;bold&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1: loss=213.89221
Tau:  39.800499167707294
Actual delays clamped:  tensor([13., 16.,  3.,  5.,  5.,  8., 10., 19.,  5.,  7., 12.,  4.,  7.,  7., 15., 17.,  6., 13.,  8., 10., 16.,  6., 16., 16.,  1., 19.,  4., 18., 14.,  8.,  0.,  1., 10.,  0., 11.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 2: loss=206.98854
Tau:  39.601993349966726
Actual delays clamped:  tensor([13., 16.,  4.,  5.,  6.,  8.,  9., 18.,  6.,  7., 10.,  4.,  6.,  6., 14., 17.,  5., 13.,  6.,  9., 15.,  5., 15., 15.,  2., 18.,  3., 18., 14.,  7.,  0.,  1., 10.,  1., 10.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 3: loss=221.05182
Tau:  39.40447758412251
Actual delays clamped:  tensor([13., 15.,  5.,  7.,  7.,  9.,  9., 18.,  6.,  7., 10.,  4.,  5.,  6., 14., 16.,  5., 12.,  5.,  9., 14.,  4., 15., 14.,  1., 18.,  2., 18., 13.,  6., -0.,  1.,  9.,  1., 10.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 4: loss=214.21939
Tau:  39.20794693227022
Actual delays clamped:  tensor([13., 15.,  6.,  7.,  8.,  9.,  9., 16.,  6.,  7.,  9.,  4.,  5.,  6., 12., 16.,  4., 11.,  5.,  8., 14.,  3., 14., 13.,  1., 17.,  1., 18., 13.,  6., -0.,  1.,  8.,  1.,  9.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 5: loss=219.21149
Tau:  39.012396481133315
Actual delays clamped:  tensor([13., 15.,  7.,  8.,  8.,  9.,  9., 15.,  6.,  7.,  9.,  4.,  5.,  5., 11., 15.,  4., 10.,  4.,  6., 13.,  2., 14., 13.,  1., 17.,  1., 17., 12.,  6., -0., -0.,  8.,  1.,  9.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 6: loss=207.44154
Tau:  38.81782134194034
Actual delays clamped:  tensor([13., 14.,  7.,  8.,  8.,  9.,  8., 15.,  6.,  7.,  8.,  5.,  5.,  5.,  9., 14.,  4.,  9.,  4.,  6., 13.,  2., 13., 13.,  1., 16.,  0., 17., 12.,  5., -1., -1.,  8.,  0.,  9.,  3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 7: loss=194.18622
Tau:  38.62421665030267
Actual delays clamped:  tensor([13., 14.,  8.,  8.,  9.,  9.,  8., 14.,  6.,  6.,  8.,  5.,  5.,  5.,  8., 13.,  4.,  8.,  3.,  4., 12.,  2., 12., 12.,  0., 16.,  0., 17., 12.,  4., -1., -1.,  8., -1.,  8.,  2.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 8: loss=201.98199
Tau:  38.43157756609294
Actual delays clamped:  tensor([13., 14.,  8.,  8.,  9.,  9.,  8., 13.,  6.,  6.,  7.,  5.,  5.,  4.,  7., 13.,  4.,  7.,  2.,  4., 11.,  1., 12., 12.,  0., 15.,  0., 17., 11.,  4., -1., -1.,  8., -1.,  8.,  1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 9: loss=189.21359
Tau:  38.239899273324006
Actual delays clamped:  tensor([13., 14.,  8.,  8.,  9.,  9.,  8., 13.,  6.,  5.,  7.,  5.,  5.,  5.,  7., 12.,  3.,  6.,  2.,  4., 10.,  1., 11., 11.,  1., 14., -1., 17., 11.,  4., -1., -2.,  7., -2.,  8.,  1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 10: loss=199.00769
Tau:  38.04917698002857
Actual delays clamped:  tensor([13., 13.,  8.,  9.,  9.,  8.,  8., 12.,  6.,  6.,  7.,  4.,  5.,  4.,  6., 12.,  3.,  5.,  2.,  3.,  9.,  1., 11., 10.,  1., 13., -2., 17., 10.,  3., -1., -2.,  7., -2.,  7.,  1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 11: loss=175.72852
Tau:  37.859405918139366
Actual delays clamped:  tensor([12., 13.,  9.,  9.,  9.,  9.,  9., 12.,  6.,  6.,  7.,  4.,  4.,  4.,  5., 11.,  3.,  5.,  2.,  3.,  7.,  1., 10., 10.,  0., 13., -2., 16.,  9.,  2., -2., -2.,  6., -2.,  6.,  1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 12: loss=164.45201
Tau:  37.67058134336996
Actual delays clamped:  tensor([12., 13.,  9.,  9.,  9.,  9.,  9., 11.,  7.,  5.,  7.,  5.,  5.,  4.,  5., 11.,  3.,  4.,  2.,  3.,  7.,  0., 10.,  9., -1., 13., -2., 16.,  9.,  2., -2., -2.,  6., -2.,  5.,  1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 13: loss=177.23515
Tau:  37.48269853509615
Actual delays clamped:  tensor([12., 13.,  9., 10.,  9.,  8.,  9., 10.,  7.,  6.,  7.,  5.,  5.,  4.,  4., 10.,  2.,  4.,  1.,  2.,  6.,  1.,  9.,  8., -1., 12., -2., 16.,  9.,  1., -4., -3.,  6., -2.,  4.,  0.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 14: loss=162.86548
Tau:  37.29575279623794
Actual delays clamped:  tensor([12., 13.,  9.,  9.,  9.,  8.,  9., 10.,  7.,  5.,  6.,  4.,  4.,  4.,  4.,  9.,  2.,  3.,  1.,  2.,  5., -0.,  8.,  7., -1., 11., -3., 15.,  8.,  1., -4., -3.,  6., -2.,  4., -0.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 15: loss=149.06389
Tau:  37.109739453142126
Actual delays clamped:  tensor([12., 13.,  9., 10.,  9.,  9.,  9.,  9.,  7.,  5.,  6.,  5.,  4.,  4.,  4.,  8.,  2.,  3.,  1.,  1.,  4., -1.,  8.,  6., -2., 11., -3., 15.,  7.,  0., -3., -3.,  5., -3.,  4., -1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 16: loss=146.48004
Tau:  36.92465385546544
Actual delays clamped:  tensor([12., 13.,  9., 10.,  9.,  9.,  9.,  9.,  7.,  5.,  6.,  4.,  5.,  4.,  3.,  7.,  2.,  2.,  1.,  1.,  4., -0.,  6.,  5., -2., 10., -4., 15.,  7.,  0., -3., -3.,  5., -3.,  4., -1.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 17: loss=162.00101
Tau:  36.74049137605831
Actual delays clamped:  tensor([13., 13., 10., 10.,  9.,  9.,  8.,  9.,  7.,  6.,  6.,  5.,  4.,  4.,  4.,  6.,  3.,  2.,  1.,  1.,  3., -1.,  5.,  4., -1.,  9., -4., 14.,  7., -0., -3., -3.,  4., -4.,  3., -3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 18: loss=155.69220
Tau:  36.55724741084914
Actual delays clamped:  tensor([12., 13., 10., 10.,  9.,  9.,  9.,  9.,  7.,  6.,  5.,  5.,  5.,  3.,  3.,  5.,  2.,  2.,  0.,  1.,  2., -1.,  4.,  3., -1.,  8., -3., 14.,  6., -1., -3., -4.,  3., -5.,  3., -3.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 19: loss=164.55096
Tau:  36.37491737872927
Actual delays clamped:  tensor([13., 12., 10., 10.,  9.,  8.,  8.,  8.,  6.,  5.,  6.,  5.,  4.,  4.,  3.,  5.,  2.,  1.,  1.,  1.,  1., -1.,  4.,  3., -2.,  7., -3., 14.,  5., -1., -3., -4.,  3., -6.,  2., -4.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 20: loss=169.56473
Tau:  36.1934967214384
Actual delays clamped:  tensor([12., 12., 11., 10.,  9.,  9.,  8.,  8.,  7.,  5.,  6.,  5.,  4.,  4.,  3.,  5.,  2.,  2.,  0.,  1.,  1., -1.,  3.,  2., -2.,  6., -3., 14.,  4., -1., -4., -4.,  2., -6.,  2., -4.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 21: loss=141.23711
Tau:  36.01298090345064
Actual delays clamped:  tensor([12., 12., 11., 10.,  9.,  8.,  8.,  8.,  6.,  5.,  6.,  4.,  4.,  4.,  3.,  4.,  2.,  1.,  0.,  1.,  1., -1.,  3.,  2., -2.,  5., -3., 13.,  3., -2., -4., -4.,  2., -6.,  1., -4.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 22: loss=121.14310
Tau:  35.833365411861145
Actual delays clamped:  tensor([12., 12., 10., 11.,  8.,  8.,  8.,  8.,  6.,  5.,  6.,  4.,  4.,  4.,  3.,  4.,  2.,  2.,  1.,  0.,  1., -2.,  2.,  1., -2.,  5., -3., 13.,  3., -2., -4., -4.,  2., -6., -0., -5.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 23: loss=126.49578
Tau:  35.65464575627327
Actual delays clamped:  tensor([12., 12., 11., 10.,  9.,  8.,  8.,  8.,  6.,  6.,  6.,  4.,  4.,  4.,  3.,  4.,  1.,  1.,  0.,  0.,  0., -2.,  1.,  0., -2.,  4., -4., 13.,  3., -3., -5., -5.,  1., -6., -0., -5.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 24: loss=127.13188
Tau:  35.47681746868631
Actual delays clamped:  tensor([13., 12., 10., 10.,  9.,  8.,  9.,  7.,  6.,  6.,  5.,  4.,  5.,  3.,  3.,  3.,  2.,  1.,  0.,  1.,  0., -1.,  0.,  0., -2.,  3., -5., 12.,  2., -3., -5., -5., -0., -7., -1., -6.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 25: loss=133.46939
Tau:  35.29987610338383
Actual delays clamped:  tensor([12., 13., 11., 10.,  9.,  9.,  9.,  7.,  6.,  5.,  5.,  4.,  4.,  4.,  3.,  3.,  1.,  1.,  0.,  0., -0., -1., -0., -0., -3.,  3., -4., 11.,  2., -3., -5., -5., -1., -8., -2., -6.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 26: loss=140.65504
Tau:  35.12381723682247
Actual delays clamped:  tensor([13., 13., 10., 10.,  9.,  8.,  9.,  7.,  6.,  6.,  6.,  4.,  4.,  3.,  3.,  3.,  1.,  1., -0., -0., -0., -1., -1., -1., -2.,  2., -4., 10., -0., -4., -5., -5., -1., -8., -3., -7.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 27: loss=127.55577
Tau:  34.94863646752139
Actual delays clamped:  tensor([13., 13., 10.,  9.,  9.,  9.,  9.,  7.,  6.,  6.,  5.,  4.,  4.,  3.,  3.,  3.,  2.,  1., -0.,  0., -0., -1., -1., -1., -3.,  1., -4.,  9., -1., -4., -5., -5., -2., -8., -3., -8.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 28: loss=115.98046
Tau:  34.774329415952245
Actual delays clamped:  tensor([13., 13., 10., 10.,  9.,  8.,  9.,  7.,  7.,  5.,  5.,  5.,  5.,  4.,  3.,  2.,  2.,  1., -0., -1., -0., -2., -1., -2., -3., -0., -4.,  8., -2., -4., -6., -5., -3., -8., -4., -8.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 29: loss=118.64677
Tau:  34.600891724429665
Actual delays clamped:  tensor([13., 12., 10.,  9.,  9.,  8.,  9.,  7.,  7.,  6.,  5.,  4.,  5.,  4.,  4.,  2.,  2.,  1., -1., -1., -1., -2., -1., -2., -3., -1., -4.,  8., -2., -5., -6., -6., -4., -8., -5., -9.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 30: loss=112.75248
Tau:  34.42831905700233
Actual delays clamped:  tensor([12., 12., 10.,  9.,  8.,  8.,  9.,  7.,  7.,  6.,  6.,  4.,  5.,  4.,  4.,  2.,  1.,  1., -0., -1., -1., -1., -2., -2., -2., -1., -5.,  8., -3., -4., -6., -6., -4., -8., -5., -9.],
       grad_fn=&lt;RoundBackward0&gt;) 





Epoch 31: loss=132.19170
Tau:  34.256607099344556
Actual delays clamped:  tensor([ 13.,  12.,  10.,   9.,   8.,   8.,   9.,   7.,   6.,   6.,   6.,   4.,   4.,   4.,   3.,   2.,   1.,   1.,  -0.,  -2.,  -1.,  -1.,  -2.,  -2.,  -3.,  -2.,  -5.,   7.,  -3.,  -5.,  -6.,  -6.,
         -4.,  -8.,  -6., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 32: loss=111.45088
Tau:  34.08575155864847
Actual delays clamped:  tensor([ 13.,  12.,  10.,  10.,   9.,   8.,   8.,   7.,   6.,   6.,   6.,   4.,   4.,   3.,   3.,   2.,   1.,   2.,  -1.,  -2.,  -1.,  -2.,  -2.,  -2.,  -3.,  -2.,  -5.,   6.,  -4.,  -5.,  -7.,  -7.,
         -5.,  -9.,  -8., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 33: loss=115.68442
Tau:  33.915748163516646
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   6.,   5.,   6.,   4.,   4.,   3.,   3.,   3.,   1.,   2.,  -0.,  -1.,  -0.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,   5.,  -4.,  -5.,  -7.,  -6.,
         -5.,  -9.,  -8., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 34: loss=125.40372
Tau:  33.74659266385536
Actual delays clamped:  tensor([ 13.,  12.,  10.,   9.,   8.,   8.,   8.,   7.,   7.,   5.,   6.,   4.,   4.,   3.,   3.,   3.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -4.,   5.,  -5.,  -6.,  -6.,  -7.,
         -5.,  -9.,  -8., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 35: loss=120.26091
Tau:  33.57828083076831
Actual delays clamped:  tensor([ 12.,  12.,  10.,   9.,   8.,   8.,   8.,   7.,   6.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,  -1.,  -2.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,   4.,  -5.,  -6.,  -6.,  -7.,
         -6.,  -9.,  -8., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 36: loss=136.04704
Tau:  33.410808456450894
Actual delays clamped:  tensor([ 13.,  12.,  10.,   9.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   2.,   1.,  -1.,  -1.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -4.,   3.,  -5.,  -6.,  -7.,  -7.,
         -6.,  -9.,  -9., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 37: loss=109.85690
Tau:  33.24417135408504
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   7.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -3.,  -3.,  -2.,  -4.,  -5.,   2.,  -5.,  -6.,  -6.,  -7.,
         -6.,  -9.,  -9., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 38: loss=117.12911
Tau:  33.078365357734505
Actual delays clamped:  tensor([ 12.,  11.,  10.,   9.,   9.,   8.,   8.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   2.,   1.,  -1.,  -1.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,   1.,  -5.,  -7.,  -6.,  -7.,
         -7., -10.,  -9., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 39: loss=113.00961
Tau:  32.91338632224075
Actual delays clamped:  tensor([ 12.,  11.,  10.,   9.,   9.,   8.,   8.,   7.,   7.,   5.,   5.,   4.,   4.,   2.,   3.,   3.,   2.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,   0.,  -5.,  -6.,  -7.,  -7.,
         -7.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 40: loss=112.69810
Tau:  32.74923012311928
Actual delays clamped:  tensor([ 12.,  12.,  11.,   9.,   9.,   8.,   7.,   7.,   7.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   2.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -1.,  -6.,  -6.,  -7.,  -7.,
         -7., -10., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 41: loss=97.33277
Tau:  32.58589265645659
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   9.,   8.,   7.,   7.,   7.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   2.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -4.,  -3.,  -4.,  -5.,  -2.,  -6.,  -6.,  -7.,  -8.,
         -7., -10., -10., -10.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 42: loss=116.12752
Tau:  32.423369838807496
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   9.,   8.,   7.,   6.,   7.,   5.,   5.,   4.,   4.,   2.,   3.,   2.,   2.,   1.,   0.,  -1.,  -1.,  -2.,  -4.,  -4.,  -3.,  -4.,  -5.,  -2.,  -6.,  -6.,  -8.,  -8.,
         -7., -10., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 43: loss=119.61383
Tau:  32.261657607093085
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   4.,   2.,   3.,   2.,   1.,   1.,   0.,  -0.,  -1.,  -3.,  -3.,  -3.,  -3.,  -4.,  -5.,  -3.,  -6.,  -6.,  -7.,  -7.,
         -8.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 44: loss=117.24825
Tau:  32.10075191849915
Actual delays clamped:  tensor([ 13.,  11.,  11.,  10.,   9.,   8.,   7.,   7.,   7.,   6.,   5.,   4.,   4.,   2.,   3.,   3.,   1.,   1.,   0.,  -0.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -4.,  -6.,  -6.,  -7.,  -7.,
         -8.,  -9.,  -9., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 45: loss=124.36443
Tau:  31.940648750375093
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   3.,   3.,   1.,   1.,   0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -4.,  -6.,  -6.,  -7.,  -7.,
         -8.,  -9.,  -9., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 46: loss=112.33761
Tau:  31.78134410013337
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   9.,   8.,   8.,   6.,   6.,   6.,   5.,   5.,   4.,   2.,   2.,   3.,   1.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -4.,  -6.,  -6.,  -7.,  -7.,
         -8.,  -9.,  -9., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 47: loss=107.57156
Tau:  31.622833985149434
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,   8.,   8.,   8.,   7.,   6.,   6.,   5.,   5.,   4.,   2.,   2.,   3.,   2.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -4.,  -6.,  -7.,  -7.,  -7.,
         -9.,  -9.,  -9., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 48: loss=93.60680
Tau:  31.46511444266215
Actual delays clamped:  tensor([ 13.,  12.,  12.,   9.,   8.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   4.,   2.,   3.,   3.,   1.,   1.,  -0.,  -0.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -8.,  -8.,
         -8.,  -9.,  -9., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 49: loss=103.69614
Tau:  31.30818152967474
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   5.,   4.,   2.,   2.,   3.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -3.,  -4.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -8.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 50: loss=110.39407
Tau:  31.15203132285621
Actual delays clamped:  tensor([ 12.,  12.,  12.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   3.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -2.,  -4.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -8., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 51: loss=102.23041
Tau:  30.996659918443253
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   8.,   8.,   7.,   6.,   6.,   6.,   4.,   3.,   3.,   3.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -4.,  -3.,  -4.,  -6.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -8., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 52: loss=87.80583
Tau:  30.842063432142666
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   0.,  -0.,  -2.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -5.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -8., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 53: loss=110.07890
Tau:  30.688237999034243
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   8.,   8.,   8.,   7.,   7.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   1.,  -0.,  -1.,  -1.,  -3.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -8., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 54: loss=106.59482
Tau:  30.53517977347414
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   8.,   7.,   7.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 55: loss=103.37586
Tau:  30.382884928998752
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   8.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -8., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 56: loss=102.83639
Tau:  30.231349658229032
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -5.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -8.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 57: loss=88.47796
Tau:  30.080570172775317
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,  10.,   8.,   7.,   8.,   7.,   5.,   5.,   5.,   3.,   3.,   2.,   2.,   2.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -5.,  -4.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -8.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 58: loss=103.27277
Tau:  29.93054270314262
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   4.,   2.,   2.,   2.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -5.,  -4.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 59: loss=109.70818
Tau:  29.781263498636385
Actual delays clamped:  tensor([ 13.,  12.,  11.,  11.,  10.,   8.,   7.,   7.,   6.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   2.,  -0.,  -0.,  -1.,  -0.,  -2.,  -2.,  -3.,  -4.,  -5.,  -4.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -8.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 60: loss=99.24034
Tau:  29.632728827268725
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   8.,   8.,   6.,   6.,   6.,   5.,   4.,   3.,   3.,   1.,   2.,  -0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -4.,  -3.,  -5.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 61: loss=93.09592
Tau:  29.48493497566512
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   8.,   7.,   6.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,  -0.,  -0.,  -0.,  -1.,  -3.,  -2.,  -3.,  -3.,  -5.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 62: loss=104.41956
Tau:  29.33787824897158
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   8.,   8.,   6.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   1.,   0.,  -1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -6.,  -7.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 63: loss=94.80013
Tau:  29.191554970762283
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   8.,   8.,   6.,   6.,   6.,   4.,   4.,   3.,   2.,   2.,   1.,   0.,  -1.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -8., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 64: loss=114.94038
Tau:  29.04596148294765
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,  10.,   8.,   8.,   8.,   6.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -6.,  -8.,  -8.,
         -8.,  -8., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 65: loss=86.27399
Tau:  28.9010941456829
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,  10.,   8.,   8.,   8.,   6.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -9.,  -8., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 66: loss=111.20496
Tau:  28.75694933727706
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,  10.,   8.,   8.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   1.,  -1.,  -1.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 67: loss=84.88992
Tau:  28.61352345410241
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   8.,   8.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   2.,   1.,  -0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 68: loss=89.06052
Tau:  28.470812910504403
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   8.,   8.,   6.,   6.,   5.,   4.,   3.,   3.,   2.,   2.,   2.,   1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -8., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 69: loss=91.34140
Tau:  28.328814138712012
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   8.,   6.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,   1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 70: loss=101.37876
Tau:  28.18752358874855
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   7.,   8.,   7.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,   1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 71: loss=94.94196
Tau:  28.04693772834291
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   7.,   8.,   7.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -6.,  -8.,  -9.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 72: loss=107.59822
Tau:  27.90705304284126
Actual delays clamped:  tensor([ 13.,  11.,  11.,  10.,   9.,   8.,   7.,   8.,   7.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,  -0.,  -1.,  -2.,  -3.,  -2.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -7.,  -8.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 73: loss=99.18828
Tau:  27.76786603511917
Actual delays clamped:  tensor([ 13.,  11.,  11.,   9.,   9.,   8.,   7.,   7.,   7.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,   0.,  -1.,  -2.,  -3.,  -2.,  -4.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 74: loss=87.21222
Tau:  27.629373225494202
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,  10.,   8.,   7.,   7.,   7.,   5.,   5.,   5.,   4.,   3.,   3.,   1.,   1.,   1.,   1.,  -1.,  -1.,  -3.,  -2.,  -4.,  -4.,  -4.,  -5.,  -6.,  -6.,  -6.,  -7.,  -8.,
         -9.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 75: loss=99.24420
Tau:  27.491571151638905
Actual delays clamped:  tensor([ 13.,  12.,  11.,   9.,  10.,   9.,   7.,   7.,   6.,   6.,   6.,   5.,   4.,   3.,   2.,   1.,   2.,   0.,   0.,  -1.,  -1.,  -3.,  -3.,  -3.,  -5.,  -4.,  -5.,  -6.,  -7.,  -6.,  -7.,  -8.,
         -9.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 76: loss=89.13180
Tau:  27.35445636849425
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,  10.,   9.,   8.,   7.,   6.,   6.,   5.,   5.,   4.,   3.,   3.,   2.,   1.,   0.,   0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -7.,  -8.,
         -9.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 77: loss=92.32360
Tau:  27.21802544818352
Actual delays clamped:  tensor([ 12.,  12.,  12.,  10.,  10.,   8.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   3.,   2.,   1.,   1.,   0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 78: loss=95.25966
Tau:  27.082274979926606
Actual delays clamped:  tensor([ 12.,  12.,  12.,   9.,  10.,   9.,   7.,   6.,   7.,   6.,   5.,   5.,   3.,   3.,   3.,   2.,   1.,   1.,   0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -7.,  -8.,
        -10.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 79: loss=93.78920
Tau:  26.94720156995472
Actual delays clamped:  tensor([ 12.,  12.,  12.,  10.,   9.,   9.,   7.,   7.,   7.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,   1.,  -0.,  -0.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -6.,  -7.,  -7.,  -8.,
        -10.,  -9., -10., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 80: loss=100.85188
Tau:  26.81280184142559
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   9.,   7.,   6.,   7.,   6.,   6.,   5.,   4.,   3.,   2.,   2.,   2.,   1.,  -0.,  -1.,  -1.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -7.,  -8.,
        -10.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 81: loss=94.46414
Tau:  26.679072434338995
Actual delays clamped:  tensor([ 12.,  12.,  12.,  10.,   9.,   9.,   6.,   7.,   7.,   6.,   6.,   5.,   4.,   4.,   2.,   1.,   2.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -9.,
        -10.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 82: loss=96.96902
Tau:  26.546010005452793
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   8.,   6.,   7.,   6.,   6.,   6.,   5.,   4.,   4.,   2.,   2.,   1.,   1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 83: loss=92.26762
Tau:  26.413611228199333
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   6.,   7.,   6.,   6.,   6.,   5.,   4.,   4.,   2.,   2.,   1.,   0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -9.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 84: loss=93.43929
Tau:  26.28187279260229
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   6.,   7.,   6.,   6.,   5.,   5.,   4.,   4.,   2.,   2.,   2.,   0.,  -1.,  -1.,  -2.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -9.,
        -10., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 85: loss=92.84187
Tau:  26.15079140519391
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   1.,  -0.,  -1.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -9.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 86: loss=86.57841
Tau:  26.02036378893268
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   9.,   7.,   7.,   7.,   5.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   0.,  -0.,  -0.,  -1.,  -2.,  -2.,  -4.,  -4.,  -4.,  -5.,  -5.,  -7.,  -6.,  -8.,  -8.,
        -10., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 87: loss=97.51117
Tau:  25.890586683121402
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   9.,   7.,   7.,   7.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -9.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 88: loss=94.93601
Tau:  25.76145684332567
Actual delays clamped:  tensor([ 13.,  11.,  11.,  10.,  10.,   9.,   7.,   7.,   6.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   0.,   0.,  -1.,  -2.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -9.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 89: loss=85.78743
Tau:  25.63297104129277
Actual delays clamped:  tensor([ 13.,  11.,  11.,  10.,  10.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   3.,   4.,   3.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 90: loss=103.67634
Tau:  25.50512606487095
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   2.,   2.,   1.,   1.,   0.,  -1.,  -2.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -8.,
         -9.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 91: loss=101.78358
Tau:  25.377918717929145
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   9.,   7.,   7.,   7.,   5.,   6.,   4.,   4.,   3.,   2.,   2.,   1.,   1.,   0.,  -1.,  -2.,  -3.,  -3.,  -4.,  -3.,  -4.,  -5.,  -5.,  -7.,  -6.,  -8.,  -9.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 92: loss=81.86739
Tau:  25.251345820277056
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   7.,   7.,   7.,   5.,   6.,   4.,   3.,   3.,   2.,   1.,   1.,   1.,   0.,  -0.,  -2.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -7.,  -6.,  -8.,  -9.,
         -9., -10., -12., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 93: loss=91.36929
Tau:  25.12540420758565
Actual delays clamped:  tensor([ 13.,  11.,  12.,  10.,  10.,   9.,   7.,   7.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   2.,   1.,   1.,   1.,  -1.,  -2.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -5.,  -7.,  -6.,  -8.,  -9.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 94: loss=87.92188
Tau:  25.00009073130805
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   9.,   7.,   7.,   7.,   6.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,   1.,   0.,  -1.,  -2.,  -1.,  -3.,  -3.,  -4.,  -4.,  -5.,  -5.,  -7.,  -6.,  -8.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 95: loss=105.20119
Tau:  24.875402258600822
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   2.,   2.,   0.,  -2.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 96: loss=90.49703
Tau:  24.751335672245652
Actual delays clamped:  tensor([ 13.,  12.,  11.,  11.,   9.,   9.,   7.,   7.,   7.,   5.,   5.,   4.,   4.,   3.,   2.,   2.,   2.,   1.,   0.,  -2.,  -1.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -6.,  -6.,  -7.,  -8.,  -8.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 97: loss=86.69691
Tau:  24.627887870571424
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   8.,   7.,   6.,   6.,   6.,   4.,   4.,   3.,   2.,   2.,   1.,   1.,   0.,  -2.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -8.,  -8.,
        -10., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 98: loss=82.09892
Tau:  24.505055767376664
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   6.,   7.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   1.,   1.,  -0.,  -2.,  -1.,  -1.,  -3.,  -4.,  -4.,  -4.,  -4.,  -6.,  -6.,  -7.,  -8.,  -8.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 99: loss=84.23597
Tau:  24.382836291852392
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   6.,   6.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -5.,  -5.,  -5.,  -7.,  -7.,  -8.,  -9.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 100: loss=86.59376
Tau:  24.261226388505357
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -4.,  -4.,  -4.,  -4.,  -5.,  -7.,  -7.,  -7.,  -9.,
         -9.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 101: loss=95.50719
Tau:  24.14022301708164
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,   1.,  -1.,  -1.,  -2.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -6.,  -6.,  -7.,  -7.,  -9.,
         -9., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 102: loss=85.65646
Tau:  24.01982315249066
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   6.,   5.,   4.,   3.,   3.,   3.,   1.,   1.,   1.,  -0.,  -1.,  -2.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -8.,  -9.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 103: loss=77.53738
Tau:  23.90002378472952
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   9.,   8.,   7.,   6.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -6.,  -6.,  -7.,  -7.,  -8.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 104: loss=88.81592
Tau:  23.780821918807792
Actual delays clamped:  tensor([ 13.,  12.,  12.,  10.,   9.,   8.,   8.,   7.,   6.,   6.,   5.,   4.,   3.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -9.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 105: loss=93.33525
Tau:  23.66221457467262
Actual delays clamped:  tensor([ 13.,  13.,  11.,  10.,   9.,   8.,   8.,   7.,   6.,   6.,   6.,   5.,   3.,   4.,   3.,   2.,   0.,   1.,  -0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -6.,  -6.,  -6.,  -8.,  -8.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 106: loss=81.96902
Tau:  23.544198787134224
Actual delays clamped:  tensor([ 12.,  13.,  12.,  10.,   9.,   9.,   8.,   7.,   6.,   5.,   6.,   5.,   3.,   3.,   3.,   1.,   1.,   1.,  -0.,  -1.,  -2.,  -3.,  -3.,  -3.,  -3.,  -4.,  -5.,  -6.,  -6.,  -7.,  -8.,  -9.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 107: loss=69.56184
Tau:  23.426771605791767
Actual delays clamped:  tensor([ 12.,  12.,  12.,  10.,   9.,   8.,   7.,   7.,   7.,   5.,   6.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,  -0.,  -1.,  -2.,  -3.,  -3.,  -3.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -8.,  -9.,
        -10., -10., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 108: loss=73.04661
Tau:  23.3099300949596
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   9.,   7.,   8.,   7.,   5.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,   0.,  -0.,  -0.,  -2.,  -3.,  -3.,  -3.,  -3.,  -5.,  -5.,  -4.,  -7.,  -7.,  -8.,  -9.,
        -10.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 109: loss=79.00967
Tau:  23.19367133359387
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   7.,   5.,   5.,   5.,   3.,   3.,   3.,   2.,   1.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -4.,  -3.,  -4.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 110: loss=91.47821
Tau:  23.077992415219484
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   2.,   1.,   2.,   1.,  -0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -5.,  -7.,  -7.,  -8.,  -9.,
        -10.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 111: loss=83.98277
Tau:  22.962890447857458
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   2.,   1.,   1.,   0.,   0.,  -0.,  -1.,  -3.,  -3.,  -3.,  -5.,  -5.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
        -10.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 112: loss=87.41134
Tau:  22.84836255395261
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   7.,   5.,   6.,   4.,   4.,   3.,   2.,   1.,   2.,   0.,   0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 113: loss=66.59123
Tau:  22.734405870301636
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   6.,   6.,   5.,   4.,   4.,   4.,   3.,   1.,   2.,   0.,   1.,  -0.,  -2.,  -3.,  -3.,  -3.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 114: loss=84.67450
Tau:  22.621017547981502
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   7.,   7.,   6.,   6.,   6.,   4.,   4.,   3.,   3.,   1.,   1.,   0.,   1.,  -0.,  -2.,  -2.,  -4.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -12., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 115: loss=74.83030
Tau:  22.508194752278246
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   4.,   4.,   2.,   2.,   1.,   1.,   1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -7.,
         -9., -10., -12., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 116: loss=75.15494
Tau:  22.3959346626161
Actual delays clamped:  tensor([ 12.,  13.,  11.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   5.,   3.,   3.,   2.,   1.,   1.,   1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -12., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 117: loss=71.63402
Tau:  22.284234472486975
Actual delays clamped:  tensor([ 12.,  13.,  11.,  10.,   9.,   8.,   8.,   8.,   7.,   6.,   5.,   4.,   5.,   3.,   3.,   1.,   1.,   1.,   0.,  -0.,  -2.,  -2.,  -3.,  -3.,  -3.,  -5.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 118: loss=81.68290
Tau:  22.1730913893803
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   8.,   7.,   8.,   6.,   6.,   5.,   5.,   5.,   3.,   2.,   1.,   1.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -4.,  -3.,  -4.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 119: loss=77.22773
Tau:  22.06250263471321
Actual delays clamped:  tensor([ 12.,  13.,  11.,  10.,   9.,   9.,   8.,   7.,   6.,   6.,   5.,   5.,   5.,   3.,   2.,   1.,   1.,   1.,   1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 120: loss=72.92089
Tau:  21.952465443761074
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   5.,   4.,   3.,   1.,   1.,   1.,   1.,  -0.,  -1.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
        -10., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 121: loss=80.86556
Tau:  21.842977065588393
Actual delays clamped:  tensor([ 12.,  12.,  11.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   4.,   3.,   1.,   1.,   1.,   0.,  -0.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 122: loss=82.20259
Tau:  21.73403476298001
Actual delays clamped:  tensor([ 12.,  13.,  11.,  10.,   9.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   4.,   2.,   2.,   2.,   1.,   0.,  -0.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -7.,  -6.,  -8.,  -8.,
        -10., -10., -12., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 123: loss=73.44653
Tau:  21.625635812372682
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   2.,   2.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -9., -10., -12., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 124: loss=75.84527
Tau:  21.517777503787
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   3.,   1.,   2.,   0.,  -0.,  -0.,  -1.,  -2.,  -3.,  -4.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -9.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 125: loss=77.23947
Tau:  21.41045714075963
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   2.,   1.,   2.,   0.,  -0.,  -0.,  -1.,  -1.,  -3.,  -4.,  -3.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,  -9.,
        -10., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 126: loss=78.21560
Tau:  21.303672040275906
Actual delays clamped:  tensor([ 13.,  12.,  10.,  10.,  10.,   9.,   8.,   8.,   6.,   6.,   5.,   4.,   4.,   4.,   3.,   1.,   1.,   1.,  -0.,  -1.,  -1.,  -1.,  -3.,  -4.,  -3.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -9.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 127: loss=74.55333
Tau:  21.197419532702757
Actual delays clamped:  tensor([ 13.,  13.,  11.,  10.,  10.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   3.,   2.,   2.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -2.,  -4.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -8.,  -9.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 128: loss=78.72210
Tau:  21.091696961721958
Actual delays clamped:  tensor([ 13.,  13.,  11.,  10.,  10.,   9.,   8.,   8.,   6.,   6.,   5.,   4.,   4.,   4.,   2.,   2.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -7.,  -9.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 129: loss=81.25073
Tau:  20.98650168426373
Actual delays clamped:  tensor([ 13.,  13.,  10.,  10.,  10.,   9.,   8.,   8.,   6.,   5.,   5.,   4.,   4.,   4.,   2.,   2.,   0.,   1.,  -1.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -5.,  -4.,  -6.,  -6.,  -7.,  -7.,  -9.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 130: loss=66.30266
Tau:  20.881831070440654
Actual delays clamped:  tensor([ 12.,  12.,  10.,  10.,   9.,   9.,   8.,   8.,   6.,   5.,   5.,   4.,   4.,   4.,   2.,   2.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -6.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 131: loss=79.05717
Tau:  20.777682503481937
Actual delays clamped:  tensor([ 13.,  12.,  10.,  10.,  10.,   9.,   8.,   8.,   6.,   5.,   5.,   5.,   4.,   4.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -7.,  -8.,
         -8.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 132: loss=82.01065
Tau:  20.67405337966798
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   9.,   8.,   7.,   7.,   6.,   5.,   4.,   4.,   4.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -5.,  -5.,  -6.,  -7.,  -7.,  -7.,  -8.,
         -8.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 133: loss=76.46279
Tau:  20.57094110826529
Actual delays clamped:  tensor([ 13.,  12.,  10.,  10.,  10.,   9.,   8.,   7.,   7.,   6.,   5.,   5.,   4.,   3.,   2.,   1.,   1.,   1.,  -0.,  -1.,  -1.,  -2.,  -2.,  -3.,  -3.,  -4.,  -4.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 134: loss=77.74532
Tau:  20.46834311146171
Actual delays clamped:  tensor([ 12.,  12.,  10.,  10.,  10.,   8.,   8.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 135: loss=87.32999
Tau:  20.366256824301978
Actual delays clamped:  tensor([ 12.,  12.,  10.,  10.,  10.,   8.,   8.,   7.,   6.,   6.,   6.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,  -1.,  -1.,  -2.,  -2.,  -2.,  -3.,  -3.,  -4.,  -5.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 136: loss=74.58962
Tau:  20.264679694623595
Actual delays clamped:  tensor([ 12.,  12.,  10.,  10.,  10.,   8.,   8.,   7.,   7.,   6.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,   1.,  -1.,  -1.,  -1.,  -2.,  -2.,  -4.,  -3.,  -4.,  -4.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 137: loss=75.00626
Tau:  20.16360918299303
Actual delays clamped:  tensor([ 12.,  12.,  10.,  10.,  10.,   9.,   8.,   7.,   6.,   6.,   6.,   4.,   4.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -1.,  -2.,  -4.,  -4.,  -4.,  -4.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 138: loss=69.72094
Tau:  20.063042762642233
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   0.,  -0.,  -0.,  -1.,  -1.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -7.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 139: loss=72.12265
Tau:  19.96297791940545
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   1.,   0.,  -0.,  -2.,  -1.,  -2.,  -4.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 140: loss=74.50313
Tau:  19.86341215165639
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,   9.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   3.,   3.,   2.,   2.,   1.,   0.,  -0.,  -1.,  -1.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 141: loss=63.87031
Tau:  19.76434297024568
Actual delays clamped:  tensor([ 13.,  12.,  11.,  10.,  10.,   8.,   8.,   7.,   7.,   6.,   5.,   5.,   3.,   3.,   3.,   2.,   1.,   0.,  -0.,  -1.,  -2.,  -2.,  -2.,  -3.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 142: loss=82.13799
Tau:  19.665767898438617
Actual delays clamped:  tensor([ 13.,  11.,  11.,  11.,  10.,   8.,   8.,   7.,   7.,   6.,   6.,   5.,   3.,   3.,   3.,   2.,   2.,   1.,  -0.,  -1.,  -1.,  -1.,  -2.,  -4.,  -4.,  -4.,  -5.,  -5.,  -6.,  -7.,  -7.,  -8.,
         -9., -10., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 143: loss=72.26529
Tau:  19.567684471853273
Actual delays clamped:  tensor([ 13.,  11.,  11.,  11.,  10.,   8.,   8.,   7.,   7.,   6.,   6.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,  -0.,  -1.,  -2.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -5.,  -6.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 144: loss=78.05957
Tau:  19.47009023839888
Actual delays clamped:  tensor([ 13.,  11.,  11.,  11.,  10.,   8.,   8.,   7.,   7.,   5.,   6.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,  -0.,  -1.,  -1.,  -1.,  -2.,  -4.,  -4.,  -3.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 145: loss=64.17740
Tau:  19.372982758214512
Actual delays clamped:  tensor([ 13.,  12.,  11.,  11.,  10.,   9.,   9.,   7.,   7.,   6.,   5.,   5.,   4.,   3.,   2.,   2.,   2.,   0.,  -0.,  -1.,  -2.,  -2.,  -2.,  -4.,  -3.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 146: loss=73.27613
Tau:  19.27635960360811
Actual delays clamped:  tensor([ 13.,  12.,  11.,  11.,  10.,   9.,   9.,   7.,   7.,   5.,   5.,   5.,   4.,   4.,   3.,   2.,   2.,   0.,   1.,  -1.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 147: loss=81.49669
Tau:  19.180218358995777
Actual delays clamped:  tensor([ 13.,  12.,  11.,  11.,   9.,   8.,   9.,   7.,   7.,   5.,   6.,   5.,   4.,   3.,   3.,   2.,   2.,   1.,   1.,  -1.,  -2.,  -1.,  -3.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -9.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 148: loss=68.30952
Tau:  19.08455662084139
Actual delays clamped:  tensor([ 13.,  13.,  11.,  11.,  10.,   8.,   9.,   7.,   7.,   5.,   5.,   5.,   4.,   3.,   3.,   2.,   1.,   1.,   0.,  -1.,  -2.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -11., -11.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 149: loss=63.83877
Tau:  18.98937199759651
Actual delays clamped:  tensor([ 13.,  13.,  10.,  11.,  10.,   9.,   9.,   7.,   6.,   5.,   5.,   4.,   4.,   3.,   2.,   2.,   1.,   1.,   0.,  -1.,  -1.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -5.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 150: loss=64.95507
Tau:  18.894662109640603
Actual delays clamped:  tensor([ 12.,  13.,  10.,  11.,  10.,   8.,   8.,   7.,   7.,   5.,   5.,   4.,   4.,   3.,   3.,   2.,   1.,   1.,   1.,  -1.,  -1.,  -2.,  -3.,  -4.,  -4.,  -4.,  -5.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -11., -12.], grad_fn=&lt;RoundBackward0&gt;) 





Epoch 151: loss=66.50641
Tau:  18.80042458922153
Actual delays clamped:  tensor([ 12.,  13.,  10.,  11.,   9.,   8.,   8.,   7.,   7.,   5.,   5.,   5.,   4.,   3.,   2.,   1.,   2.,   0.,   1.,  -1.,  -2.,  -2.,  -2.,  -4.,  -4.,  -4.,  -4.,  -6.,  -7.,  -7.,  -8.,  -8.,
         -8.,  -9., -10., -12.], grad_fn=&lt;RoundBackward0&gt;) 
</pre></div>
</div>
<img alt="../_images/Solving_problem_with_delay_learning_18_1.png" src="../_images/Solving_problem_with_delay_learning_18_1.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
</pre></div>
</div>
<img alt="../_images/Solving_problem_with_delay_learning_18_3.png" src="../_images/Solving_problem_with_delay_learning_18_3.png" />
<img alt="../_images/Solving_problem_with_delay_learning_18_4.png" src="../_images/Solving_problem_with_delay_learning_18_4.png" />
</div>
</div>
</div>
<div class="section" id="performance-measure-one">
<h2>Performance measure one<a class="headerlink" href="#performance-measure-one" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Test performance</span>
<span class="n">trial_count</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">test_array</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">trial_count</span><span class="p">):</span>  <span class="c1"># An algorithm to compute the difference between the prediction and target in degrees</span>
    <span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">get_batch</span><span class="p">())</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">snn_sl</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">max_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">target</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">))</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">diff_pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">target</span><span class="o">-</span><span class="n">max_index</span><span class="p">)</span>
    <span class="n">test_array</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">diff_pred</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
<span class="n">diff_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_array</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">diff_pred</span><span class="o">*</span><span class="n">ANG_STEP</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Testing bacth diff. between predictions in degree&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Difference (degree)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Count (au)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Solving_problem_with_delay_learning_20_0.png" src="../_images/Solving_problem_with_delay_learning_20_0.png" />
</div>
</div>
</div>
<div class="section" id="performance-measure-two">
<h2>Performance measure two<a class="headerlink" href="#performance-measure-two" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Analyse (taken from the starting notebook)</span>
<span class="k">def</span> <span class="nf">chunker</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>  <span class="c1"># A function to iterate over a list/array in fixed sizes</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">seq</span><span class="p">[</span><span class="n">pos</span><span class="p">:</span><span class="n">pos</span> <span class="o">+</span> <span class="n">size</span><span class="p">]</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">seq</span><span class="p">),</span> <span class="n">size</span><span class="p">))</span>

<span class="n">XS_TRAIN</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">X_TRAIN</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span> <span class="p">))</span>
<span class="n">YS_TRAIN</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Y_TRAIN</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span> <span class="p">)</span>
<span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">increments</span> <span class="o">=</span> <span class="n">XS_TRAIN</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2000</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="n">increments</span>
<span class="n">YS_TEST</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">XS_TEST</span><span class="p">,</span> <span class="n">Y_TEST</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">get_batch</span><span class="p">())</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
    <span class="n">YS_TEST</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Y_TEST</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">YS_TEST</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">YS_TEST</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">analyse</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">ipds</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">spikes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">run</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualization of the results of training</span>

<span class="sd">    Parameters:</span>
<span class="sd">        label(string): Type of inputs (train or test)</span>
<span class="sd">        ipds(numpy.ndarray, (BATCH_SIZE, NUMBER_CLASSES)): A batch of targets in whole number form</span>
<span class="sd">        spikes(torch.Tensor, (BATCH_SIZE, NUMBER_CLASSES, NUMBER_INPUTS, EFFECTIVE_DURATION)): A batch of input spike trains</span>
<span class="sd">        run: A callable function</span>

<span class="sd">    Returns:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spikes</span> <span class="o">=</span> <span class="n">spikes</span><span class="p">[:</span><span class="n">increments</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">ipds</span> <span class="o">=</span> <span class="n">ipds</span><span class="p">[:</span><span class="n">increments</span><span class="p">]</span>
    <span class="n">confusion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">NUMBER_CLASSES</span><span class="p">))</span>
    <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ipds_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="n">ANG_STEP</span><span class="p">)</span>
    <span class="n">ipds_true_deg</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ipds_est_deg</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">chunker</span><span class="p">(</span><span class="n">spikes</span><span class="p">,</span> <span class="n">increments</span><span class="p">):</span>  <span class="c1"># Chunk an array into fixed sizes</span>
        <span class="n">output_temp</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_temp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span> <span class="p">))</span>
    <span class="n">ipds_true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">ipds</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">ipds_est</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># argmax over output units</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">ipds_true</span> <span class="o">==</span> <span class="n">ipds_est</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># compare to labels</span>
    <span class="n">ipds_true</span> <span class="o">=</span> <span class="n">ipds_true</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">ipds_est</span> <span class="o">=</span> <span class="n">ipds_est</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ipds_true</span><span class="p">,</span> <span class="n">ipds_est</span><span class="p">):</span>  <span class="c1"># Generate the confusion matrix</span>
        <span class="n">confusion</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">ipds_true_deg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ipds_range</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)])</span>
        <span class="n">ipds_est_deg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ipds_range</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">j</span><span class="p">)])</span>
    <span class="n">ipds_true_deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ipds_true_deg</span><span class="p">)</span>
    <span class="n">ipds_est_deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ipds_est_deg</span><span class="p">)</span>
    <span class="n">abs_errors_deg</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">ipds_true_deg</span><span class="o">-</span><span class="n">ipds_est_deg</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> classifier accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> absolute error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_errors_deg</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> deg&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ipds_true_deg</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ipds_est_deg</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">NUMBER_CLASSES</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;IPD&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">confusion</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True IPD&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimated IPD&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>    

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Chance accuracy level: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="mi">1</span><span class="o">/</span><span class="n">NUMBER_CLASSES</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="n">run_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">snn_sl</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">analyse</span><span class="p">(</span><span class="s1">&#39;Train&#39;</span><span class="p">,</span> <span class="n">ipds</span><span class="o">=</span><span class="n">YS_TRAIN</span><span class="p">,</span> <span class="n">spikes</span><span class="o">=</span><span class="n">XS_TRAIN</span><span class="p">,</span> <span class="n">run</span><span class="o">=</span><span class="n">run_func</span><span class="p">)</span>
<span class="n">analyse</span><span class="p">(</span><span class="s1">&#39;Test&#39;</span><span class="p">,</span> <span class="n">ipds</span><span class="o">=</span><span class="n">YS_TEST</span><span class="p">,</span> <span class="n">spikes</span><span class="o">=</span><span class="n">XS_TEST</span><span class="p">,</span> <span class="n">run</span><span class="o">=</span><span class="n">run_func</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Chance accuracy level: 2.8%

Train classifier accuracy: 10.9%
Train absolute error: 19.5 deg

Test classifier accuracy: 11.8%
Test absolute error: 19.1 deg
</pre></div>
</div>
<img alt="../_images/Solving_problem_with_delay_learning_22_1.png" src="../_images/Solving_problem_with_delay_learning_22_1.png" />
<img alt="../_images/Solving_problem_with_delay_learning_22_2.png" src="../_images/Solving_problem_with_delay_learning_22_2.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Workshop_1_Write_Up.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Workshop 1 Write-up</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="SNN_sound_W1W2_threshold_plot.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Modified from starting Notebook</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By COMOB, the project for collaborative modelling of the brain<br/>
    
      <div class="extra_footer">
        <small>
  Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA</a>:
  You may use this work, with attribution, in other freely available works.
</small>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>