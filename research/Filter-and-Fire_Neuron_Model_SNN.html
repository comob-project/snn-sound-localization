
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Filter-and-Fire Neuron Model &#8212; SNN Sound Localization</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style-mods.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://comob-project.github.io/snn-sound-localization/research/Filter-and-Fire_Neuron_Model_SNN.html" />
    <link rel="shortcut icon" href="../_static/headphone-logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sound localisation with excitatory-only inputs surrogate gradient descent" href="Excitatory-only-localisation.html" />
    <link rel="prev" title="&lt;no title&gt;" href="Learning_delays.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">SNN Sound Localization</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   About
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Contributing.html">
   How to contribute
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/comob-project/snn-sound-localization/discussions/categories/q-a">
   Discussion forum
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Research
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Background.html">
   Background
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Questions.html">
   Questions &amp; challenges
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Starting-Notebook.html">
   Starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Workshop_1_Write_Up.html">
   Workshop 1 Write-up
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="SNN_sound_W1W2_threshold_plot.html">
   Modified from starting Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Optimizing-Membrane-Time-Constant.html">
   Improving Performance: Optimizing the membrane time constant
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Quick_Start.html">
   Quick Start Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Learning_delays_major_edit2.html">
   Learning delays
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Filter-and-Fire Neuron Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Excitatory-only-localisation.html">
   Sound localisation with excitatory-only inputs surrogate gradient descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dynamic_threshold.html">
   Dynamic threshold
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Dales_law.html">
   Sound localisation following Dale’ law
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Compute%20hessians%20%28jax%20version%29.html">
   Compute hessians (jax version)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Analysing-Trained-Networks.html">
   (WIP) Analysing trained networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Analysing-Trained-Networks-Part2.html">
   Analysing trained networks - workshop edition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Altering_output_neurons.html">
   Altering Output Neurons
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/research/Filter-and-Fire_Neuron_Model_SNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/comob-project/snn-sound-localization"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        <a class="edit-button" href="https://github.com/comob-project/snn-sound-localization/edit/main/research/Filter-and-Fire_Neuron_Model_SNN.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/comob-project/snn-sound-localization/main?urlpath=tree/research/Filter-and-Fire_Neuron_Model_SNN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/comob-project/snn-sound-localization/blob/main/research/Filter-and-Fire_Neuron_Model_SNN.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ideas-to-adapt-filter-and-fire-neuron-model-in-the-sound-localisation-snn-step-by-step">
     Ideas to adapt filter-and-fire neuron model in the sound localisation SNN (step-by-step)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-dos">
   TO-DOs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions">
   Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stimulus">
     Stimulus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#snn">
     SNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test-performance-of-the-trained-model">
     Train and Test Performance of the Trained Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Filter-and-Fire Neuron Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-1-training-testing-results">
   Exp 1. Training &amp; Testing Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimal-input-smoothing-fast-tau-rise-and-tau-decay">
     Minimal Input Smoothing (fast
     <code class="docutils literal notranslate">
      <span class="pre">
       tau_rise
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       tau_decay
      </span>
     </code>
     )
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-input-smoothing">
     Normal Input Smoothing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-2-training-testing-results">
   Exp 2. Training &amp; Testing Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-usual-phase-delays">
     Using the usual phase delays
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-no-phase-delays">
     Using no phase delays
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-3-training-testing-results">
   Exp 3. Training &amp; Testing Results
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Filter-and-Fire Neuron Model</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background">
   Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ideas-to-adapt-filter-and-fire-neuron-model-in-the-sound-localisation-snn-step-by-step">
     Ideas to adapt filter-and-fire neuron model in the sound localisation SNN (step-by-step)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#to-dos">
   TO-DOs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#functions">
   Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stimulus">
     Stimulus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#snn">
     SNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-and-test-performance-of-the-trained-model">
     Train and Test Performance of the Trained Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Filter-and-Fire Neuron Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-1-training-testing-results">
   Exp 1. Training &amp; Testing Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#minimal-input-smoothing-fast-tau-rise-and-tau-decay">
     Minimal Input Smoothing (fast
     <code class="docutils literal notranslate">
      <span class="pre">
       tau_rise
      </span>
     </code>
     and
     <code class="docutils literal notranslate">
      <span class="pre">
       tau_decay
      </span>
     </code>
     )
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#normal-input-smoothing">
     Normal Input Smoothing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-2-training-testing-results">
   Exp 2. Training &amp; Testing Results
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-the-usual-phase-delays">
     Using the usual phase delays
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-no-phase-delays">
     Using no phase delays
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exp-3-training-testing-results">
   Exp 3. Training &amp; Testing Results
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="filter-and-fire-neuron-model">
<h1>Filter-and-Fire Neuron Model<a class="headerlink" href="#filter-and-fire-neuron-model" title="Permalink to this headline">¶</a></h1>
<div class="section" id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>In this notebook, we aim to use a different biological neuron model than the LIF neuron used in the SNN trained for the sound localisation task.</p>
<p><a class="reference external" href="https://www.biorxiv.org/content/10.1101/2022.01.28.478132v2.abstract">Beniaguev et al. (2022, preprint)</a> recently proposed the filter-and-fire neuron, which takes inspiration from the neurobiological finding that presynaptic neurons make more than one synaptic contact on the dendrites of their “target” postsynaptic neuron. These synaptic contacts can be made at various locations on the dendritic tree of the postsynaptic neuron, i.e., at various distances from the neuron’s soma (see Figure 1 in the paper).</p>
<p>Below, we highlight the sequential steps in which we aim to introduce/adapt this neuron model in this project. You can have a look at these couple of slides for some high-level overview: <a class="reference external" href="https://docs.google.com/presentation/d/1XMqWLDBRnLCNdStXknKGdnDYZfWRdgFE2QmzbV0vbgk/edit?usp=sharing">Google Slides</a>.</p>
<p>In brief, we think the presence of multiple synaptic connections at different distances on the dendritic tree of the postsynaptic neuron can be useful for the project’s training.</p>
<ul class="simple">
<li><p>With multiple synaptic connections made by the same presynaptic neuron’s axon, we get individual PSPs (i.e., for each contact that axon makes on the postsynaptic dendrites) with <em>different temporal profiles</em>.</p>
<ul>
<li><p>the further from the postsynaptic neuron’s soma a synaptic connection is, the “broader” the temporal profile of the resulting individual PSP from that connection is.</p></li>
<li><p>these PSPs with different temporal profiles from one presynaptic axon are summed to make up the postsynaptic neuron’s somatic response for inputs coming from that axon.</p></li>
</ul>
</li>
<li><p>We can tune the weights at the synaptic contacts made by each presynaptic neuron’s axon to adjust the magnitude and timing of the peak of the summed postsynaptic somatic response for each presynaptic axon.</p></li>
<li><p>We hypothesise that this will increase performance by allowing the network to tune the delays associated with different inputs (compared to, e.g., training the delays).</p></li>
</ul>
<div class="section" id="ideas-to-adapt-filter-and-fire-neuron-model-in-the-sound-localisation-snn-step-by-step">
<h3>Ideas to adapt filter-and-fire neuron model in the sound localisation SNN (step-by-step)<a class="headerlink" href="#ideas-to-adapt-filter-and-fire-neuron-model-in-the-sound-localisation-snn-step-by-step" title="Permalink to this headline">¶</a></h3>
<p>Note:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">presynaptic_input_spikes_tensor</span></code>: the original spike train inputs</p></li>
<li><p>tau: synaptic time constant</p></li>
<li><p>M: number of connections per presynaptic axon. This is &gt; 1 in Exp 3 only.</p></li>
</ul>
<p>Main idea: Apply smoothing to the <code class="docutils literal notranslate"><span class="pre">presynaptic_input_spikes_tensor</span></code> to get PSP-like inputs, by convolving the original input spike trains with a normalised filter.</p>
<ul class="simple">
<li><p>Exp 1. convolve input_spikes with one single filter, with fixed tau hyperparameters (use Beniaguev code to create filter shape, turn this into tensor)</p></li>
<li><p>Exp 2. convolve input_spikes with different, random filters (try with, and without delay ranges = a random fixed delay value or just 0); here, we don’t use fixed tau hyperparameters and instead randomly sample them from predefined ranges.</p></li>
<li><p>Exp 3. use filter-and-fire neurons (e.g. M = 3 connections per axon)</p></li>
</ul>
</div>
</div>
<div class="section" id="to-dos">
<h2>TO-DOs<a class="headerlink" href="#to-dos" title="Permalink to this headline">¶</a></h2>
<p>(As of end of Dec 2022)</p>
<ul class="simple">
<li><p>Section on Exp 3.</p>
<ul>
<li><p>action: I have an implementation of this (trained it too) but need to check it before adding it to the end of this notebook.</p></li>
<li><p>action: once an implementation has been confirmed to work appropriately, I will train and test this for more epochs using UCL’s compute.</p></li>
</ul>
</li>
<li><p>Exp 2.</p>
<ul>
<li><p>is relatively poor performance of random taus because, on average, the PSP widths are wider than in the single, fixed tau case in Exp 1 (i.e. too much smoothing with the random filters)?</p>
<ul>
<li><p>action: train for more epochs (will do with UCL’s compute in January)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Training duration</p>
<ul>
<li><p>action: in general, I need to train for more than 100 epochs (will do with UCL’s compute in January)</p></li>
</ul>
</li>
<li><p>Hyperparameter tuning</p>
<ul>
<li><p>action: can do a search to optimise the hyperparameters (esp. the tau hyperparameters, to find the optimal synaptic time constants) for example</p></li>
<li><p>action: can test out <a class="reference external" href="https://wandb.ai/site">Weights and Biases</a></p></li>
</ul>
</li>
<li><p>Other extension ideas</p>
<ul>
<li><p>examples: to improve the performance; to gain more insight into the network…</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>tqdm

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span> <span class="k">as</span> <span class="n">pbar</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>

<span class="c1"># Check whether a GPU is available</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>     
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    
<span class="n">my_computer_is_slow</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># set this to True if using Colab</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Constants</span>
<span class="n">SECOND</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MS</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">HZ</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">PI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
<span class="n">DT</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="n">MS</span> <span class="c1"># large time step to make simulations run faster</span>

<span class="n">ANF_PER_EAR</span> <span class="o">=</span> <span class="mi">100</span>    <span class="c1"># repeats of each ear with independent noise (was 1000 in my other notebooks)</span>
<span class="n">ENVELOPE_POWER</span> <span class="o">=</span> <span class="mi">2</span>   <span class="c1"># higher values make sharper envelopes, easier</span>
<span class="n">RATE_MAX</span> <span class="o">=</span> <span class="mi">600</span><span class="o">*</span><span class="n">HZ</span>   <span class="c1"># maximum Poisson firing rate</span>
<span class="n">F</span> <span class="o">=</span> <span class="mi">20</span><span class="o">*</span><span class="n">HZ</span>           <span class="c1"># stimulus frequency</span>
<span class="n">DURATION</span> <span class="o">=</span> <span class="mf">.1</span><span class="o">*</span><span class="n">SECOND</span> <span class="c1"># stimulus duration</span>
<span class="n">DURATION_STEPS</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">DURATION</span><span class="o">/</span><span class="n">DT</span><span class="p">))</span> <span class="c1"># 100</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">DURATION_STEPS</span><span class="p">)</span><span class="o">*</span><span class="n">DT</span> <span class="c1"># array of times</span>

<span class="c1"># Network</span>
<span class="n">INPUT_SIZE</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">ANF_PER_EAR</span> <span class="c1"># 200 input neurons</span>
<span class="n">NUM_HIDDEN</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">180</span><span class="o">//</span><span class="mi">15</span> <span class="c1"># classes at 15 degree increments</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of classes = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">NUM_CLASSES</span><span class="p">))</span>
<span class="n">TAU</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">MS</span> <span class="c1"># this used to be 20 in the SNN Starting Notebook, is now 5 in Quick Start Notebook</span>

<span class="c1"># Training</span>
<span class="n">MY_COMPUTER_IS_SLOW</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">if</span> <span class="n">MY_COMPUTER_IS_SLOW</span><span class="p">:</span>
    <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">N_TRAINING_BATCHES</span> <span class="o">=</span> <span class="mi">64</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">N_TRAINING_BATCHES</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">N_TESTING_BATCHES</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">NUM_SAMPLES</span> <span class="o">=</span> <span class="n">BATCH_SIZE</span><span class="o">*</span><span class="n">N_TRAINING_BATCHES</span>
<span class="n">BETA</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># for Surrogate Gradient Descent</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># (takes ~6min/epoch with INPUT_SIZE=200)</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="c1"># Filter-and-Fire Neuron Model (Beniaguev et al., 2022)</span>
<span class="n">NUM_AXONS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">CONNECTIONS_PER_AXON</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">NUM_SYNAPSES</span> <span class="o">=</span> <span class="n">CONNECTIONS_PER_AXON</span> <span class="o">*</span> <span class="n">NUM_AXONS</span>
 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of classes = 12
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="functions">
<h2>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="stimulus">
<h3>Stimulus<a class="headerlink" href="#stimulus" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">input_signal</span><span class="p">(</span><span class="n">ipd</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate an input signal (spike array) from array of true IPDs</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipd (array): true IPDs</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    spikes (array): input signal from true IPDs (spike trains)</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">num_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ipd</span><span class="p">)</span> <span class="c1"># i.e., NUM_SAMPLES</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">PI</span><span class="o">*</span><span class="p">(</span><span class="n">F</span><span class="o">*</span><span class="n">T</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span> <span class="c1"># array of phases corresponding to those times with random offset</span>
 
    <span class="c1"># each point in the array will have a different phase based on which ear it is</span>
    <span class="c1"># and its delay</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">DURATION_STEPS</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ANF_PER_EAR</span><span class="p">))</span>
    
    <span class="c1"># for each ear, we have anf_per_ear different phase delays from to pi/2 so</span>
    <span class="c1"># that the differences between the two ears can cover the full range from -pi/2 to pi/2</span>
    <span class="k">if</span> <span class="n">usual_phase_delays</span><span class="p">:</span> <span class="c1"># Exp 1., Exp 2.a</span>
        <span class="n">phase_delays</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">PI</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">ANF_PER_EAR</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># Exp 2.b, Exp 3.</span>
        <span class="n">phase_delays</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ANF_PER_EAR</span><span class="p">)</span>

    <span class="c1"># now we set up these theta to implement that. Some numpy vectorisation logic here which looks a little weird,</span>
    <span class="c1"># but implements the idea in the text above.</span>
    <span class="n">theta</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">ANF_PER_EAR</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">+</span><span class="n">phase_delays</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">theta</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">ANF_PER_EAR</span><span class="p">:]</span> <span class="o">=</span> <span class="n">phi</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">+</span><span class="n">phase_delays</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span><span class="o">+</span><span class="n">ipd</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>

    <span class="c1"># now generate Poisson spikes at the given firing rate as in the previous notebook</span>
    <span class="n">spikes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">DURATION_STEPS</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">ANF_PER_EAR</span><span class="p">)</span><span class="o">&lt;</span><span class="n">RATE_MAX</span><span class="o">*</span><span class="n">DT</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">theta</span><span class="p">)))</span><span class="o">**</span><span class="n">ENVELOPE_POWER</span>

    <span class="k">return</span> <span class="n">spikes</span>

<span class="c1"># Generate some true IPDs from U(-pi/2, pi/2) and corresponding spike arrays</span>
<span class="k">def</span> <span class="nf">random_ipd_input_signal</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate the training data: true IPDs are in U(-pi/2, pi/2) </span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    num_samples (int)</span>
<span class="sd">    usual_phase_delays (boolean): flag on the use of usual or null phase delays</span>
<span class="sd">    tensor (boolean): flag on the use of tensor or numpy objects for the objects returned</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    ipd (array): true IPDs from U(-pi/2, pi/2)</span>
<span class="sd">    spikes (array): input signal corresponding to the true IPDs in the training data</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">ipd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span><span class="o">*</span><span class="n">PI</span><span class="o">-</span><span class="n">PI</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># uniformly random in (-pi/2, pi/2)</span>
    <span class="c1">#print(ipd)# okay</span>
    <span class="n">spikes</span> <span class="o">=</span> <span class="n">input_signal</span><span class="p">(</span><span class="n">ipd</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="p">)</span>
    <span class="c1">#print(spikes) # empty</span>
    
    <span class="k">if</span> <span class="n">tensor</span><span class="p">:</span>
        <span class="n">ipd</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">ipd</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>        
        <span class="n">spikes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">spikes</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">ipd</span><span class="p">,</span> <span class="n">spikes</span>


<span class="k">def</span> <span class="nf">discretise</span><span class="p">(</span><span class="n">ipds</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discretise the ipds in the training data </span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipds (tensor)</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    tensor</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">ipds</span><span class="o">+</span><span class="n">PI</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">NUM_CLASSES</span><span class="o">/</span><span class="n">PI</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span> <span class="c1"># assumes input is tensor</span>

<span class="k">def</span> <span class="nf">continuise</span><span class="p">(</span><span class="n">ipd_indices</span><span class="p">):</span> <span class="c1"># convert indices back to IPD midpoints</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Undo the discretisation of ipds</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipd_indices (array)</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    array</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">ipd_indices</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="o">/</span><span class="n">NUM_CLASSES</span><span class="o">*</span><span class="n">PI</span><span class="o">-</span><span class="n">PI</span><span class="o">/</span><span class="mi">2</span>


<span class="c1"># Generator function iterates over the data in batches</span>
<span class="c1"># We randomly permute the order of the data to improve learning</span>
<span class="k">def</span> <span class="nf">data_generator</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate the whole training data by iterating over the data in the batches.</span>
<span class="sd">    Order of the data is randomly permuted to improve learning.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipds (tensor)</span>
<span class="sd">    spikes (tensor): flag on the use of usual or null phase delays</span>
<span class="sd">  </span>
<span class="sd">    Yields:</span>
<span class="sd">    x_local (tensor): relates to the spikes</span>
<span class="sd">    y_local (tensor): relates to the ipds</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">spikes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">spikes</span> <span class="o">=</span> <span class="n">spikes</span><span class="p">[</span><span class="n">perm</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">ipds</span> <span class="o">=</span> <span class="n">ipds</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_batch</span> <span class="o">=</span> <span class="n">n</span><span class="o">//</span><span class="n">BATCH_SIZE</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_batch</span><span class="p">):</span>
        <span class="n">x_local</span> <span class="o">=</span> <span class="n">spikes</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">BATCH_SIZE</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">y_local</span> <span class="o">=</span> <span class="n">ipds</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">BATCH_SIZE</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">BATCH_SIZE</span><span class="p">]</span>
        
        <span class="k">yield</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">y_local</span>

<span class="c1"># Plot a few just to show how it looks</span>
<span class="k">def</span> <span class="nf">plot_some_input_examples</span><span class="p">(</span><span class="n">num_examples</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots some example inputs.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    num_examples (int): default = 8</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">ipd</span><span class="p">,</span> <span class="n">spikes</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
    <span class="n">spikes</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_examples</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">spikes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray_r</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;True IPD = </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">ipd</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">180</span><span class="o">/</span><span class="n">PI</span><span class="p">)</span><span class="si">}</span><span class="s1"> deg&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;=</span><span class="mi">4</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="k">4</span>==0:
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Input neuron index&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    
<span class="n">plot_some_input_examples</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_13_0.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_13_0.png" />
</div>
</div>
</div>
<div class="section" id="snn">
<h3>SNN<a class="headerlink" href="#snn" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SurrGradSpike</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="nb">input</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="k">return</span> <span class="n">out</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="nb">input</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="c1"># Original SPyTorch/SuperSpike gradient</span>
        <span class="c1"># This seems to be a typo or error? But it works well</span>
        <span class="c1">#grad = grad_output/(100*torch.abs(input)+1.0)**2</span>
        <span class="c1"># Sigmoid</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">*</span><span class="n">BETA</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">BETA</span><span class="o">*</span><span class="nb">input</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">BETA</span><span class="o">*</span><span class="nb">input</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">grad</span>

<span class="n">spike_fn</span>  <span class="o">=</span> <span class="n">SurrGradSpike</span><span class="o">.</span><span class="n">apply</span>

<span class="c1"># Run the simulation</span>
<span class="k">def</span> <span class="nf">snn</span><span class="p">(</span><span class="n">presynaptic_input_spikes_tensor</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> 
        <span class="n">multiple_connections_per_axon</span><span class="p">,</span>
        <span class="n">random_tau_constants</span><span class="p">,</span> 
        <span class="n">minimal_smoothing</span><span class="p">):</span>
<span class="w">    </span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs the SNN simulation.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    presynaptic_input_spikes_tensor (tensor): corresponds to x_local in the training</span>
<span class="sd">    W1 (tensor): initialised trainable weight parameter for the first layer</span>
<span class="sd">    W1_bis (tensor): fixed non-trainable weight parameter for the first layer when there are multiple connections per axon</span>
<span class="sd">    W2 (tensor): initialised trainable weight parameter for the second layer</span>
<span class="sd">    multiple_connections_per_axon (boolean): flag on the use of multiple connections per axon</span>
<span class="sd">    random_tau_constants (boolean): flag on the use of randomly sampled tau hyperparameters (rise and decay)</span>
<span class="sd">    minimal_smoothing: flag on the use of fast tau hyperparameters for minimal input spike smoothing</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    v_rec (tensor): recorded membrane potential of output</span>
<span class="sd">    input_smoothed (tensor): smoothed inputs</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># First layer: input to hidden</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">s_rec</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">]</span>

    <span class="c1"># Get smoothed inputs (binary spike train -&gt; PSP shapes)</span>
    <span class="n">input_smoothed</span> <span class="o">=</span> <span class="n">getDoubleExpFilteredSpikes_allBatchExamples</span><span class="p">(</span><span class="n">presynaptic_input_spikes_tensor</span><span class="p">,</span> 
                                                                 <span class="n">random_tau_constants</span><span class="p">,</span>
                                                                 <span class="n">minimal_smoothing</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">multiple_connections_per_axon</span><span class="p">:</span> <span class="c1"># for Exp 3.</span>
        <span class="n">pre_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;abc,cd-&gt;abd&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">input_smoothed</span><span class="p">,</span> <span class="n">W1</span><span class="p">))</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;abc,cd-&gt;abd&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">pre_h</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">))</span> <span class="c1"># (BATCH_SIZE, DURATION_STEPS, NUM_HIDDEN)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;abc,cd-&gt;abd&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">input_smoothed</span><span class="p">,</span> <span class="n">W1</span><span class="p">))</span>
    
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">DT</span><span class="o">/</span><span class="n">TAU</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">DURATION_STEPS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  
        <span class="n">new_v</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="n">h</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:])</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">s</span><span class="p">)</span> <span class="c1"># multiply by 0 after a spike (similar to soma_current) </span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">spike_fn</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># threshold of 1</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">new_v</span>
        <span class="n">s_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">s_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">s_rec</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Second layer: hidden to output</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">v_rec</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">]</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;abc,cd-&gt;abd&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">s_rec</span><span class="p">,</span> <span class="n">W2</span><span class="p">))</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">DT</span><span class="o">/</span><span class="n">TAU</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">DURATION_STEPS</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>  
        <span class="n">v</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="n">h</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">v_rec</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="n">v_rec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">v_rec</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Return recorded membrane potential of output and smoothed input (for visualisation)</span>
    <span class="k">return</span> <span class="n">v_rec</span><span class="p">,</span> <span class="n">input_smoothed</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Weights and uniform weight initialisation</span>
<span class="k">def</span> <span class="nf">init_weight_matrix</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialises the weight matrix used in the fanin-fanout calculations for the initialisation of W1 and W2.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    None</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    W (tensor): weight tensor used in fanin-fanout calculations</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">W</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">INPUT_SIZE</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">W</span>

<span class="k">def</span> <span class="nf">init_weight_matrices</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialises the weight matrices in the SNN: W1, W1_bis and W2.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    W (tensor): weight tensor used in fanin-fanout calculations for W1 and W2</span>
<span class="sd">    multiple_connections_per_axon (boolean): flag on the use of multiple connections per axon</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    W1 (tensor): initialised trainable weight parameter for the first layer</span>
<span class="sd">    W1_bis (tensor): fixed non-trainable weight parameter for the first layer when there are multiple connections per axon</span>
<span class="sd">    W2 (tensor): initialised trainable weight parameter for the second layer</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">multiple_connections_per_axon</span><span class="p">:</span> <span class="c1"># Exp. 3</span>
        <span class="c1"># Input to hidden layer</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">INPUT_SIZE</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="o">*</span><span class="n">CONNECTIONS_PER_AXON</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
    
        <span class="c1"># don&#39;t make W1_bis trainable</span>
        <span class="n">W1_bis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_HIDDEN</span><span class="o">*</span><span class="n">CONNECTIONS_PER_AXON</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W1_bis</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">W1_bis</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">W1_bis</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># excluding 0</span>
                <span class="n">W1_bis</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="mi">2</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span> 
    
        <span class="c1"># Hidden layer to output</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">NUM_HIDDEN</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Input to hidden layer</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">INPUT_SIZE</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
        
        <span class="c1"># placeholder for W1_bis (not used)</span>
        <span class="n">W1_bis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">NUM_HIDDEN</span><span class="o">*</span><span class="n">CONNECTIONS_PER_AXON</span><span class="p">,</span> <span class="n">NUM_HIDDEN</span><span class="p">)</span>
    
        <span class="c1"># Hidden layer to output</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">NUM_HIDDEN</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2</span>
    
    
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Runs the training of the SNN simulation.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    usual_phase_delays (boolean): flag on the use of usual or null phase delays</span>
<span class="sd">    multiple_connections_per_axon (boolean): flag on the use of multiple connections per axon</span>
<span class="sd">    random_tau_constants (boolean): flag on the use of randomly sampled tau hyperparameters (rise and decay)</span>
<span class="sd">    minimal_smoothing (boolean): flag on the use of fast tau hyperparameters for minimal input spike smoothing</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    ipds (tensor): training data (y)</span>
<span class="sd">    spikes (tensor): training data (X)</span>
<span class="sd">    W1 (tensor): trained W1</span>
<span class="sd">    W2 (tensor): trained W2</span>
<span class="sd">    snn_training_snapshot (list): snapshots of the training at each epoch</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Generate the training data</span>
    <span class="c1">#ipds, spikes = random_ipd_input_signal(NUM_SAMPLES, usual_phase_delays)</span>
    
    <span class="c1"># Initialise weight matrices</span>
    <span class="n">W</span> <span class="o">=</span> <span class="n">init_weight_matrix</span><span class="p">()</span> <span class="c1"># for fan_in/out calculations</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">init_weight_matrices</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="p">)</span>

    <span class="c1"># Optimiser and loss function</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
    <span class="n">log_softmax_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

    <span class="n">loss_hist</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">snn_training_snapshot</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">NUM_EPOCHS</span><span class="p">)):</span>
        
        <span class="n">local_loss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_number</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--------------------------------------------------------&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EPOCH </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">e</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">y_local</span> <span class="ow">in</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">discretise</span><span class="p">(</span><span class="n">ipds</span><span class="p">),</span> <span class="n">spikes</span><span class="p">):</span>
            
            <span class="n">batch_number</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="c1"># Run network</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">input_smoothed</span> <span class="o">=</span> <span class="n">snn</span><span class="p">(</span><span class="n">x_local</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="p">,</span> <span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">)</span>
            
            <span class="c1"># Compute cross entropy loss</span>
            <span class="c1">#m = torch.sum(output, 1)*0.01  # Sum time dimension</span>
            <span class="c1"># Compute cross entropy loss</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Mean across time dimension</span>

            <span class="n">reg</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># to add regularisation later if wanted</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">log_softmax_fn</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="n">y_local</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span>
            <span class="n">local_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># Update gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Record and print the loss of the current epoch</span>
        <span class="n">loss_hist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">local_loss</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;---EPOCH </span><span class="si">%i</span><span class="s2">: LOSS=</span><span class="si">%.5f</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">e</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">local_loss</span><span class="p">)))</span>

        <span class="c1"># Plot raster plot: </span>
        <span class="c1"># e.g. input_smoothed for the 1st example of the last batch of the current epoch</span>
        <span class="c1">#print(&quot;---Raster Plots at Epoch {} - Example #1/64 of the Last Batch Group&quot;.format(e+1))</span>
        <span class="c1">#plt.figure(1)</span>
        <span class="c1">#plt.title(&quot;input_smoothed example vs corresponding initial spike train&quot;)</span>
        <span class="c1">#plt.subplot(211)</span>
        <span class="c1">#plt.imshow(input_smoothed[0, :, :].detach().numpy().T, aspect=&#39;auto&#39;, interpolation=&#39;nearest&#39;, cmap=plt.cm.gray_r)</span>
        <span class="c1">#plt.xlabel(&#39;Time (steps)&#39;)</span>
        <span class="c1">#plt.ylabel(&#39;Input neuron index&#39;)</span>
        <span class="c1">#plt.subplot(212)</span>
        <span class="c1">#plt.imshow(x_local[0, :, :].detach().numpy().T, aspect=&#39;auto&#39;, interpolation=&#39;nearest&#39;, cmap=plt.cm.gray_r)</span>
        <span class="c1">#plt.xlabel(&#39;Time (steps)&#39;)</span>
        <span class="c1">#plt.ylabel(&#39;Input neuron index&#39;)</span>
    
        <span class="c1">#plt.show()</span>
        
        <span class="c1"># Take a snapshot of the model at the end of the current epoch. </span>
        <span class="c1">## Use cases:</span>
        <span class="c1">### if we want to resume training from this current epoch,</span>
        <span class="c1">### or if the training is halted before the last epoch</span>
        <span class="n">snn_training_snapshot</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;W1&#39;</span><span class="p">:</span><span class="n">W1</span><span class="p">,</span> <span class="s1">&#39;W2&#39;</span><span class="p">:</span><span class="n">W2</span><span class="p">,</span> 
                                      <span class="s1">&#39;multiple_connections_per_axon&#39;</span><span class="p">:</span><span class="n">multiple_connections_per_axon</span><span class="p">,</span> 
                                      <span class="s1">&#39;random_tau_constants&#39;</span><span class="p">:</span><span class="n">random_tau_constants</span><span class="p">,</span> 
                                      <span class="s1">&#39;minimal_smoothing&#39;</span><span class="p">:</span><span class="n">minimal_smoothing</span><span class="p">})</span> 
        
    <span class="c1"># Plot the loss function over time</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="c1"># return the train dataset (ipds, spikes) and trained weights for analysis</span>
    <span class="c1"># also return the list of model snapshots for each epoch (in case we want to resume training)</span>
    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">snn_training_snapshot</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-and-test-performance-of-the-trained-model">
<h3>Train and Test Performance of the Trained Model<a class="headerlink" href="#train-and-test-performance-of-the-trained-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_accuracy</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">,</span> <span class="n">run</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the accuracy on data (train or test)</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipds (tensor)</span>
<span class="sd">    spikes (tensor)</span>
<span class="sd">    run (lambda function)</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    ipd_true (list)</span>
<span class="sd">    ipd_est (list)</span>
<span class="sd">    confusion (numpy array)</span>
<span class="sd">    accs (list)</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ipd_true</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">ipd_est</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">confusion</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">))</span>
    
    <span class="c1">#print(ipds.shape)</span>
    <span class="c1">#print(spikes.shape)</span>
    
    <span class="k">for</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">y_local</span> <span class="ow">in</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">):</span>
        <span class="n">y_local_orig</span> <span class="o">=</span> <span class="n">y_local</span>
        <span class="n">y_local</span> <span class="o">=</span> <span class="n">discretise</span><span class="p">(</span><span class="n">y_local</span><span class="p">)</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">x_local</span><span class="p">)</span>
        <span class="c1">#m = torch.sum(output, 1)  # Sum time dimension</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">am</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># argmax over output units</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_local</span> <span class="o">==</span> <span class="n">am</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>  <span class="c1"># compare to labels</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y_local</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">am</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()):</span>
            <span class="n">confusion</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            
        <span class="n">ipd_true</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_local_orig</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">ipd_est</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">continuise</span><span class="p">(</span><span class="n">am</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
        <span class="n">accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>

    <span class="n">ipd_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">ipd_true</span><span class="p">)</span>
    <span class="n">ipd_est</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">ipd_est</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ipd_true</span><span class="p">,</span> <span class="n">ipd_est</span><span class="p">,</span> <span class="n">confusion</span><span class="p">,</span> <span class="n">accs</span>

<span class="k">def</span> <span class="nf">report_accuracy</span><span class="p">(</span><span class="n">ipd_true</span><span class="p">,</span> <span class="n">ipd_est</span><span class="p">,</span> <span class="n">confusion</span><span class="p">,</span> <span class="n">accs</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the accuracy on data (train or test).</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipd_true (list)</span>
<span class="sd">    ipd_est (list)</span>
<span class="sd">    confusion (numpy array)</span>
<span class="sd">    accs (list)</span>
<span class="sd">    label (string): &quot;Test&quot; or &quot;Train&quot;</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">abs_errors_deg</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">ipd_true</span><span class="o">-</span><span class="n">ipd_est</span><span class="p">)</span><span class="o">*</span><span class="mi">180</span><span class="o">/</span><span class="n">PI</span>

    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> classifier accuracy: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> absolute error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">abs_errors_deg</span><span class="p">)</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2"> deg&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ipd_true</span> <span class="o">*</span> <span class="mi">180</span> <span class="o">/</span> <span class="n">PI</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">ipd_est</span> <span class="o">*</span> <span class="mi">180</span> <span class="o">/</span> <span class="n">PI</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;IPD&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
    <span class="n">confusion</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">confusion</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="o">-</span><span class="mi">90</span><span class="p">,</span> <span class="mi">90</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True IPD&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Estimated IPD&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion matrix&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>    

<span class="k">def</span> <span class="nf">analyse_accuracy</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">,</span> <span class="n">W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2_trained</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="p">,</span><span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Analyses the accuracy on data (train or test)</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    ipds_train (tensor)</span>
<span class="sd">    spikes_train (tensor)</span>
<span class="sd">    W1_trained (tensor): trained W1</span>
<span class="sd">    W2_trained (tensor): trained W2</span>
<span class="sd">    multiple_connections_per_axon (boolean): flag on the use of multiple connections per axon</span>
<span class="sd">    random_tau_constants (boolean): flag on the use of randomly sampled tau hyperparameters (rise and decay)</span>
<span class="sd">    minimal_smoothing (boolean): flag on the use of fast tau hyperparameters for minimal input spike smoothing</span>
<span class="sd">    test_data (boolean): flag on the use of test data for the analysis of the accuracy</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    ipd_true (list)</span>
<span class="sd">    ipd_est (list)</span>
<span class="sd">    confusion (numpy array)</span>
<span class="sd">    accs (list)</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">run_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">snn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">W2_trained</span><span class="p">,</span> <span class="n">multiple_connections_per_axon</span><span class="p">,</span><span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">)</span>
    
    <span class="n">ipd_true</span><span class="p">,</span> <span class="n">ipd_est</span><span class="p">,</span> <span class="n">confusion</span><span class="p">,</span> <span class="n">accs</span> <span class="o">=</span> <span class="n">get_accuracy</span><span class="p">(</span><span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span><span class="p">,</span> <span class="n">run_function</span><span class="p">)</span>
    
    <span class="c1"># Analyse test accuracy</span>
    <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span> 
        <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Test&quot;</span>
    <span class="c1"># Analyse train accuracy</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Train&quot;</span>

    <span class="n">report_accuracy</span><span class="p">(</span><span class="n">ipd_true</span><span class="p">,</span> <span class="n">ipd_est</span><span class="p">,</span> <span class="n">confusion</span><span class="p">,</span> <span class="n">accs</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

    <span class="k">return</span> <span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Filter-and-Fire Neuron Model<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getRandomTauHyperparameters</span><span class="p">(</span><span class="n">tau_rise_range</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="n">tau_decay_range</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">]):</span> <span class="c1"># also add fast ones</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the randomly sampled tau hyperparameters.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    tau_rise_range (list): default = [2,10]</span>
<span class="sd">    tau_decay_range (list): default = [10,20]</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    tau_rise (int)</span>
<span class="sd">    tau_decay (float)</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">tau_rise</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">tau_rise_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">tau_rise_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">tau_decay</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">tau_decay_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">tau_decay_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="c1"># we don&#39;t want tau_rise to be bigger than tau_decay * safety_factor</span>
    <span class="n">safety_factor</span> <span class="o">=</span> <span class="mf">1.5</span>
    <span class="k">if</span> <span class="n">tau_rise</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">tau_decay</span> <span class="o">/</span> <span class="n">safety_factor</span><span class="p">):</span>
        <span class="n">tau_decay</span> <span class="o">=</span> <span class="n">safety_factor</span> <span class="o">*</span> <span class="n">tau_rise</span>

    <span class="k">return</span> <span class="n">tau_rise</span><span class="p">,</span> <span class="n">tau_decay</span>

<span class="k">def</span> <span class="nf">getDoubleExpFilteredSpikes_singleBatchExample</span><span class="p">(</span><span class="n">spikes</span><span class="p">,</span> <span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the PSP shapes (smoothed input) for a single batch example.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    spikes (tensor): </span>
<span class="sd">    random_tau_constants (boolean): flag on the use of randomly sampled tau hyperparameters</span>
<span class="sd">    minimal_smoothing (boolean): flag on the use of fast tau hyperparameters for minimal input spike smoothing</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    smoothed (tensor): smoothed input for a single batch example</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Define the tau and smoothing parameters</span>
    <span class="k">if</span> <span class="n">random_tau_constants</span><span class="p">:</span> <span class="c1"># Exp 2., Exp 3.</span>
        <span class="n">tau_rise</span><span class="p">,</span> <span class="n">tau_decay</span> <span class="o">=</span> <span class="n">getRandomTauHyperparameters</span><span class="p">()</span>
        <span class="n">wd</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">tau_decay</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># Exp 1.</span>
        <span class="k">if</span> <span class="n">minimal_smoothing</span><span class="p">:</span> <span class="c1"># fast tau hyperparameters to preserve &quot;spikiness&quot;</span>
            <span class="n">tau_rise</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">,</span> <span class="n">wd</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">5</span> 
        <span class="k">else</span><span class="p">:</span> <span class="c1"># normal smoothing to get PSPs </span>
            <span class="n">tau_rise</span><span class="p">,</span> <span class="n">tau_decay</span><span class="p">,</span> <span class="n">wd</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span>
    <span class="c1"># note: wd is the window size for the smoothing (in timesteps),</span>
    <span class="c1"># i.e. width (timesteps) of the smoothing to the left and the right of the spike timestep </span>
    
    <span class="n">spikes_numpy</span> <span class="o">=</span> <span class="n">spikes</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># (1,100,2000) because doing it for one example at a time</span>
    
    <span class="c1"># Define the double exponential kernel to obtain PSPs</span>
    <span class="c1"># See Book Chapter &quot;Modeling Synapses&quot; Equation (6.4) p.143</span>
    <span class="c1"># link: https://www.researchgate.net/profile/Mark-Van-Rossum/publication/266712567_Modeling_Synapses/links/546389350cf2c0c6aec4e4fe/Modeling-Synapses.pdf</span>
    <span class="n">times</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">spikes_numpy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># array from 0 to 100 (step=1)</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">times</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="o">+</span><span class="n">wd</span><span class="p">]))</span><span class="o">/</span><span class="n">tau_decay</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">times</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="o">+</span><span class="n">wd</span><span class="p">]))</span><span class="o">/</span><span class="n">tau_rise</span><span class="p">)</span> 
    
    <span class="c1"># kernel normalisation</span>
    <span class="n">kernel_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_max</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">kernel</span> <span class="o">/=</span> <span class="n">kernel_max</span> <span class="c1"># normalise so that it peaks at 1 </span>

    <span class="c1"># Prepare spike window</span>
    <span class="n">spike_window</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">spikes_numpy</span><span class="p">[</span><span class="o">-</span><span class="n">wd</span><span class="p">:,:],</span> <span class="n">spikes_numpy</span><span class="p">,</span> <span class="n">spikes_numpy</span><span class="p">[:</span><span class="n">wd</span><span class="p">,:]))</span>
    <span class="c1">#print(spikeWindow.shape) # (3,100,2000)</span>
    
    <span class="c1"># Prepare smoothed array</span>
    <span class="n">n_steps</span><span class="p">,</span> <span class="n">n_neurons</span> <span class="o">=</span> <span class="n">spikes_numpy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">spikes_numpy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">spikes_numpy</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c1"># Add smoothing to every spike</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
            <span class="c1"># Only add kernel if there is a spike, otherwise don&#39;t change anything</span>
            <span class="k">if</span> <span class="n">spike_window</span><span class="p">[:,</span><span class="n">t</span><span class="p">,</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">t</span> <span class="o">+</span> <span class="n">wd</span> <span class="o">&lt;=</span> <span class="n">n_steps</span><span class="p">:</span> <span class="c1"># there is space left</span>
                    <span class="n">smoothed</span><span class="p">[:,</span><span class="n">t</span><span class="p">:</span><span class="n">t</span><span class="o">+</span><span class="n">wd</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">kernel</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1"># there is no space left</span>
                    <span class="n">updated_wd</span> <span class="o">=</span> <span class="n">n_steps</span> <span class="o">-</span> <span class="n">t</span>
                    <span class="n">updated_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">times</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="o">+</span><span class="n">updated_wd</span><span class="p">]))</span><span class="o">/</span><span class="n">tau_decay</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">times</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="o">+</span><span class="n">updated_wd</span><span class="p">]))</span><span class="o">/</span><span class="n">tau_rise</span><span class="p">)</span>
                    <span class="c1">#updated_kernel /= np.max(updated_kernel)</span>
                    <span class="n">smoothed</span><span class="p">[:,</span><span class="n">t</span><span class="p">:</span><span class="n">t</span><span class="o">+</span><span class="n">updated_wd</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">+=</span> <span class="n">updated_kernel</span>
                    
    <span class="c1"># Return smoothed activity</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">smoothed</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">getDoubleExpFilteredSpikes_allBatchExamples</span><span class="p">(</span><span class="n">presynaptic_input_spikes_tensor</span><span class="p">,</span> <span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the PSP shapes (smoothed input) for all batch examples within a batch.</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    presynaptic_input_spikes_tensor (tensor): </span>
<span class="sd">    random_tau_constants (boolean): flag on the use of randomly sampled tau hyperparameters</span>
<span class="sd">    minimal_smoothing (boolean): flag on the use of fast tau hyperparameters for minimal input spike smoothing</span>
<span class="sd">  </span>
<span class="sd">    Returns:</span>
<span class="sd">    input_smoothed (tensor): smoothed input for all batch examples of the current batch</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Prepare smoothed array with the first dimension including all batch examples</span>
    <span class="n">input_smoothed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">presynaptic_input_spikes_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># (64,100,2000)</span>
    
    <span class="c1"># Do the smoothing for each example of the current batch (64 total)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">):</span>
        <span class="n">input_smoothed_onebatch</span> <span class="o">=</span> <span class="n">getDoubleExpFilteredSpikes_singleBatchExample</span><span class="p">(</span><span class="n">presynaptic_input_spikes_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,:,:],</span> <span class="n">random_tau_constants</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="p">)</span> 
        <span class="n">input_smoothed</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,:,:]</span> <span class="o">=</span> <span class="n">input_smoothed_onebatch</span>

    <span class="k">return</span> <span class="n">input_smoothed</span>

<span class="k">def</span> <span class="nf">plot_one_smoothed_neuron_activity</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the smoothed activity of one input neuron in the first example of the first batch.</span>
<span class="sd">    E.g., 18th neuron out of 2000 neurons, 1st example out of 64, batch number 1</span>
<span class="sd">  </span>
<span class="sd">    Parameters:</span>
<span class="sd">    None</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">  </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Generate the training data</span>
    <span class="n">ipds</span><span class="p">,</span> <span class="n">spikes</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">NUM_SAMPLES</span><span class="p">)</span>
       
    <span class="n">batch_number</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">x_local</span><span class="p">,</span> <span class="n">y_local</span> <span class="ow">in</span> <span class="n">data_generator</span><span class="p">(</span><span class="n">discretise</span><span class="p">(</span><span class="n">ipds</span><span class="p">),</span> <span class="n">spikes</span><span class="p">):</span>
        <span class="n">batch_number</span><span class="o">+=</span><span class="mi">1</span>
    
        <span class="c1"># Show a single time series (for any of the 2000 input neurons and for any batch example)</span>
        <span class="c1"># e.g. 17th neuron, 1st batch example of batch number 1</span>
        <span class="k">if</span> <span class="n">batch_number</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> 
            <span class="c1"># smoothing for a single input spike train (e.g. the first spike train out of the 64 examples in the current batch group)</span>
            <span class="n">input_smoothed</span> <span class="o">=</span> <span class="n">getDoubleExpFilteredSpikes_singleBatchExample</span><span class="p">(</span><span class="n">x_local</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,:,:],</span> <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
        
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Example Spike Train #1/64 of Neuron #17 of Batch Group #</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch_number</span><span class="p">))</span> 
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">input_smoothed</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">17</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Magnitude&#39;</span><span class="p">)</span>
    
            <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_local</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">17</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (steps)&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Magnitude&#39;</span><span class="p">)</span>
    
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">plot_one_smoothed_neuron_activity</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Example Spike Train #1/64 of Neuron #17 of Batch Group #1
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/g4/79lhk_yn70dchy3qtnjksyf40000gn/T/ipykernel_41049/86159179.py:147: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:2985.)
  plt.plot(x_local[0, :, 17].T)
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_21_2.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_21_2.png" />
</div>
</div>
<hr class="docutils" />
<p>Note: if the network performs at chance, the accuracy should be 1 out of 12 (since there are 12 classes).</p>
</div>
</div>
<div class="section" id="exp-1-training-testing-results">
<h2>Exp 1. Training &amp; Testing Results<a class="headerlink" href="#exp-1-training-testing-results" title="Permalink to this headline">¶</a></h2>
<div class="section" id="minimal-input-smoothing-fast-tau-rise-and-tau-decay">
<h3>Minimal Input Smoothing (fast <code class="docutils literal notranslate"><span class="pre">tau_rise</span></code> and <code class="docutils literal notranslate"><span class="pre">tau_decay</span></code>)<a class="headerlink" href="#minimal-input-smoothing-fast-tau-rise-and-tau-decay" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training</span>
<span class="c1"># Generate the training data</span>
<span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">NUM_SAMPLES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp1Fast_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1Fast_W2_trained</span><span class="p">,</span> <span class="n">Exp1Fast_snn_training_snapshot</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span>
                                                                                         <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                                         <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                                                                         <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "22812e52f9eb40a1850d01dc97e223e6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------
EPOCH 1
---EPOCH 1: LOSS=2.58870
--------------------------------------------------------
EPOCH 2
---EPOCH 2: LOSS=2.48156
--------------------------------------------------------
EPOCH 3
---EPOCH 3: LOSS=2.43496
--------------------------------------------------------
EPOCH 4
---EPOCH 4: LOSS=2.40582
--------------------------------------------------------
EPOCH 5
---EPOCH 5: LOSS=2.37650
--------------------------------------------------------
EPOCH 6
---EPOCH 6: LOSS=2.34811
--------------------------------------------------------
EPOCH 7
---EPOCH 7: LOSS=2.31757
--------------------------------------------------------
EPOCH 8
---EPOCH 8: LOSS=2.28411
--------------------------------------------------------
EPOCH 9
---EPOCH 9: LOSS=2.25157
--------------------------------------------------------
EPOCH 10
---EPOCH 10: LOSS=2.22167
--------------------------------------------------------
EPOCH 11
---EPOCH 11: LOSS=2.19168
--------------------------------------------------------
EPOCH 12
---EPOCH 12: LOSS=2.16257
--------------------------------------------------------
EPOCH 13
---EPOCH 13: LOSS=2.13346
--------------------------------------------------------
EPOCH 14
---EPOCH 14: LOSS=2.10730
--------------------------------------------------------
EPOCH 15
---EPOCH 15: LOSS=2.08129
--------------------------------------------------------
EPOCH 16
---EPOCH 16: LOSS=2.05597
--------------------------------------------------------
EPOCH 17
---EPOCH 17: LOSS=2.03081
--------------------------------------------------------
EPOCH 18
---EPOCH 18: LOSS=2.00708
--------------------------------------------------------
EPOCH 19
---EPOCH 19: LOSS=1.98363
--------------------------------------------------------
EPOCH 20
---EPOCH 20: LOSS=1.96077
--------------------------------------------------------
EPOCH 21
---EPOCH 21: LOSS=1.93846
--------------------------------------------------------
EPOCH 22
---EPOCH 22: LOSS=1.91688
--------------------------------------------------------
EPOCH 23
---EPOCH 23: LOSS=1.89563
--------------------------------------------------------
EPOCH 24
---EPOCH 24: LOSS=1.87402
--------------------------------------------------------
EPOCH 25
---EPOCH 25: LOSS=1.85369
--------------------------------------------------------
EPOCH 26
---EPOCH 26: LOSS=1.83426
--------------------------------------------------------
EPOCH 27
---EPOCH 27: LOSS=1.81447
--------------------------------------------------------
EPOCH 28
---EPOCH 28: LOSS=1.79519
--------------------------------------------------------
EPOCH 29
---EPOCH 29: LOSS=1.77644
--------------------------------------------------------
EPOCH 30
---EPOCH 30: LOSS=1.75820
--------------------------------------------------------
EPOCH 31
---EPOCH 31: LOSS=1.73967
--------------------------------------------------------
EPOCH 32
---EPOCH 32: LOSS=1.72163
--------------------------------------------------------
EPOCH 33
---EPOCH 33: LOSS=1.70517
--------------------------------------------------------
EPOCH 34
---EPOCH 34: LOSS=1.68827
--------------------------------------------------------
EPOCH 35
---EPOCH 35: LOSS=1.67196
--------------------------------------------------------
EPOCH 36
---EPOCH 36: LOSS=1.65588
--------------------------------------------------------
EPOCH 37
---EPOCH 37: LOSS=1.64002
--------------------------------------------------------
EPOCH 38
---EPOCH 38: LOSS=1.62476
--------------------------------------------------------
EPOCH 39
---EPOCH 39: LOSS=1.61007
--------------------------------------------------------
EPOCH 40
---EPOCH 40: LOSS=1.59524
--------------------------------------------------------
EPOCH 41
---EPOCH 41: LOSS=1.58028
--------------------------------------------------------
EPOCH 42
---EPOCH 42: LOSS=1.56519
--------------------------------------------------------
EPOCH 43
---EPOCH 43: LOSS=1.55120
--------------------------------------------------------
EPOCH 44
---EPOCH 44: LOSS=1.53770
--------------------------------------------------------
EPOCH 45
---EPOCH 45: LOSS=1.52368
--------------------------------------------------------
EPOCH 46
---EPOCH 46: LOSS=1.51113
--------------------------------------------------------
EPOCH 47
---EPOCH 47: LOSS=1.49838
--------------------------------------------------------
EPOCH 48
---EPOCH 48: LOSS=1.48603
--------------------------------------------------------
EPOCH 49
---EPOCH 49: LOSS=1.47353
--------------------------------------------------------
EPOCH 50
---EPOCH 50: LOSS=1.46125
--------------------------------------------------------
EPOCH 51
---EPOCH 51: LOSS=1.44869
--------------------------------------------------------
EPOCH 52
---EPOCH 52: LOSS=1.43747
--------------------------------------------------------
EPOCH 53
---EPOCH 53: LOSS=1.42572
--------------------------------------------------------
EPOCH 54
---EPOCH 54: LOSS=1.41432
--------------------------------------------------------
EPOCH 55
---EPOCH 55: LOSS=1.40258
--------------------------------------------------------
EPOCH 56
---EPOCH 56: LOSS=1.39167
--------------------------------------------------------
EPOCH 57
---EPOCH 57: LOSS=1.38142
--------------------------------------------------------
EPOCH 58
---EPOCH 58: LOSS=1.37083
--------------------------------------------------------
EPOCH 59
---EPOCH 59: LOSS=1.36061
--------------------------------------------------------
EPOCH 60
---EPOCH 60: LOSS=1.35022
--------------------------------------------------------
EPOCH 61
---EPOCH 61: LOSS=1.34029
--------------------------------------------------------
EPOCH 62
---EPOCH 62: LOSS=1.32994
--------------------------------------------------------
EPOCH 63
---EPOCH 63: LOSS=1.32075
--------------------------------------------------------
EPOCH 64
---EPOCH 64: LOSS=1.31106
--------------------------------------------------------
EPOCH 65
---EPOCH 65: LOSS=1.30165
--------------------------------------------------------
EPOCH 66
---EPOCH 66: LOSS=1.29216
--------------------------------------------------------
EPOCH 67
---EPOCH 67: LOSS=1.28295
--------------------------------------------------------
EPOCH 68
---EPOCH 68: LOSS=1.27380
--------------------------------------------------------
EPOCH 69
---EPOCH 69: LOSS=1.26500
--------------------------------------------------------
EPOCH 70
---EPOCH 70: LOSS=1.25667
--------------------------------------------------------
EPOCH 71
---EPOCH 71: LOSS=1.24813
--------------------------------------------------------
EPOCH 72
---EPOCH 72: LOSS=1.23927
--------------------------------------------------------
EPOCH 73
---EPOCH 73: LOSS=1.23117
--------------------------------------------------------
EPOCH 74
---EPOCH 74: LOSS=1.22316
--------------------------------------------------------
EPOCH 75
---EPOCH 75: LOSS=1.21505
--------------------------------------------------------
EPOCH 76
---EPOCH 76: LOSS=1.20718
--------------------------------------------------------
EPOCH 77
---EPOCH 77: LOSS=1.19975
--------------------------------------------------------
EPOCH 78
---EPOCH 78: LOSS=1.19206
--------------------------------------------------------
EPOCH 79
---EPOCH 79: LOSS=1.18393
--------------------------------------------------------
EPOCH 80
---EPOCH 80: LOSS=1.17668
--------------------------------------------------------
EPOCH 81
---EPOCH 81: LOSS=1.16898
--------------------------------------------------------
EPOCH 82
---EPOCH 82: LOSS=1.16211
--------------------------------------------------------
EPOCH 83
---EPOCH 83: LOSS=1.15421
--------------------------------------------------------
EPOCH 84
---EPOCH 84: LOSS=1.14764
--------------------------------------------------------
EPOCH 85
---EPOCH 85: LOSS=1.14051
--------------------------------------------------------
EPOCH 86
---EPOCH 86: LOSS=1.13349
--------------------------------------------------------
EPOCH 87
---EPOCH 87: LOSS=1.12758
--------------------------------------------------------
EPOCH 88
---EPOCH 88: LOSS=1.12014
--------------------------------------------------------
EPOCH 89
---EPOCH 89: LOSS=1.11403
--------------------------------------------------------
EPOCH 90
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---EPOCH 90: LOSS=1.10702
--------------------------------------------------------
EPOCH 91
---EPOCH 91: LOSS=1.10080
--------------------------------------------------------
EPOCH 92
---EPOCH 92: LOSS=1.09409
--------------------------------------------------------
EPOCH 93
---EPOCH 93: LOSS=1.08788
--------------------------------------------------------
EPOCH 94
---EPOCH 94: LOSS=1.08179
--------------------------------------------------------
EPOCH 95
---EPOCH 95: LOSS=1.07512
--------------------------------------------------------
EPOCH 96
---EPOCH 96: LOSS=1.06909
--------------------------------------------------------
EPOCH 97
---EPOCH 97: LOSS=1.06289
--------------------------------------------------------
EPOCH 98
---EPOCH 98: LOSS=1.05764
--------------------------------------------------------
EPOCH 99
---EPOCH 99: LOSS=1.05107
--------------------------------------------------------
EPOCH 100
---EPOCH 100: LOSS=1.04531
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_26_3.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_26_3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training and testing accuracies</span>
<span class="n">Exp1Fast_train_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span> <span class="n">Exp1Fast_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1Fast_W2_trained</span><span class="p">,</span> 
                                           <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                           <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                           <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                           <span class="n">test_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Generate the test data</span>
<span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="o">*</span><span class="n">N_TESTING_BATCHES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp1Fast_test_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span><span class="p">,</span> <span class="n">Exp1Fast_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1Fast_W2_trained</span><span class="p">,</span> 
                                          <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                          <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                          <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                          <span class="n">test_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train classifier accuracy: 83.1%
Train absolute error: 4.6 deg

Test classifier accuracy: 77.4%
Test absolute error: 5.0 deg
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_27_1.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_27_1.png" />
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_27_2.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_27_2.png" />
</div>
</div>
<p>When minimal smoothing is applied, the network performs well as expected. The SNN here actually performs better than the SNN in the Starting Notebook on the TEST dataset (i.e., generalises better). The SNN Starting Notebook had the following: train accuracy = 82.9% (4.7 degree absolute error) and test accuracy = <strong>54.8%</strong> (7.9 degree absolute error). Could this tell us that some degree of smoothing on the input spike trains is actually beneficial, e.g. by removing some of the noise coming from the sparse input spike trains? Or, could some small degree of smoothing allow for a larger window of coincidence detection between two inputs while not completely sacrificing temporal information (given spike trains are more temporally precise).</p>
</div>
<div class="section" id="normal-input-smoothing">
<h3>Normal Input Smoothing<a class="headerlink" href="#normal-input-smoothing" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training</span>
<span class="c1"># Generate the training data</span>
<span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">NUM_SAMPLES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp1_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1_W2_trained</span><span class="p">,</span> <span class="n">snn_training_snapshot</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span>
                                                                        <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                        <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                                                        <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "16a2143a2ce844aaa8c16b00ba9fbac9", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------
EPOCH 1
---EPOCH 1: LOSS=2.70504
--------------------------------------------------------
EPOCH 2
---EPOCH 2: LOSS=2.53907
--------------------------------------------------------
EPOCH 3
---EPOCH 3: LOSS=2.45401
--------------------------------------------------------
EPOCH 4
---EPOCH 4: LOSS=2.40007
--------------------------------------------------------
EPOCH 5
---EPOCH 5: LOSS=2.35672
--------------------------------------------------------
EPOCH 6
---EPOCH 6: LOSS=2.31837
--------------------------------------------------------
EPOCH 7
---EPOCH 7: LOSS=2.28152
--------------------------------------------------------
EPOCH 8
---EPOCH 8: LOSS=2.24591
--------------------------------------------------------
EPOCH 9
---EPOCH 9: LOSS=2.21480
--------------------------------------------------------
EPOCH 10
---EPOCH 10: LOSS=2.18274
--------------------------------------------------------
EPOCH 11
---EPOCH 11: LOSS=2.15206
--------------------------------------------------------
EPOCH 12
---EPOCH 12: LOSS=2.11962
--------------------------------------------------------
EPOCH 13
---EPOCH 13: LOSS=2.08574
--------------------------------------------------------
EPOCH 14
---EPOCH 14: LOSS=2.05578
--------------------------------------------------------
EPOCH 15
---EPOCH 15: LOSS=2.02729
--------------------------------------------------------
EPOCH 16
---EPOCH 16: LOSS=2.00019
--------------------------------------------------------
EPOCH 17
---EPOCH 17: LOSS=1.97152
--------------------------------------------------------
EPOCH 18
---EPOCH 18: LOSS=1.94387
--------------------------------------------------------
EPOCH 19
---EPOCH 19: LOSS=1.91675
--------------------------------------------------------
EPOCH 20
---EPOCH 20: LOSS=1.89031
--------------------------------------------------------
EPOCH 21
---EPOCH 21: LOSS=1.86505
--------------------------------------------------------
EPOCH 22
---EPOCH 22: LOSS=1.84056
--------------------------------------------------------
EPOCH 23
---EPOCH 23: LOSS=1.81589
--------------------------------------------------------
EPOCH 24
---EPOCH 24: LOSS=1.79182
--------------------------------------------------------
EPOCH 25
---EPOCH 25: LOSS=1.76936
--------------------------------------------------------
EPOCH 26
---EPOCH 26: LOSS=1.74715
--------------------------------------------------------
EPOCH 27
---EPOCH 27: LOSS=1.72534
--------------------------------------------------------
EPOCH 28
---EPOCH 28: LOSS=1.70448
--------------------------------------------------------
EPOCH 29
---EPOCH 29: LOSS=1.68387
--------------------------------------------------------
EPOCH 30
---EPOCH 30: LOSS=1.66472
--------------------------------------------------------
EPOCH 31
---EPOCH 31: LOSS=1.64555
--------------------------------------------------------
EPOCH 32
---EPOCH 32: LOSS=1.62712
--------------------------------------------------------
EPOCH 33
---EPOCH 33: LOSS=1.60907
--------------------------------------------------------
EPOCH 34
---EPOCH 34: LOSS=1.59198
--------------------------------------------------------
EPOCH 35
---EPOCH 35: LOSS=1.57530
--------------------------------------------------------
EPOCH 36
---EPOCH 36: LOSS=1.55979
--------------------------------------------------------
EPOCH 37
---EPOCH 37: LOSS=1.54432
--------------------------------------------------------
EPOCH 38
---EPOCH 38: LOSS=1.52922
--------------------------------------------------------
EPOCH 39
---EPOCH 39: LOSS=1.51462
--------------------------------------------------------
EPOCH 40
---EPOCH 40: LOSS=1.49998
--------------------------------------------------------
EPOCH 41
---EPOCH 41: LOSS=1.48671
--------------------------------------------------------
EPOCH 42
---EPOCH 42: LOSS=1.47287
--------------------------------------------------------
EPOCH 43
---EPOCH 43: LOSS=1.46012
--------------------------------------------------------
EPOCH 44
---EPOCH 44: LOSS=1.44675
--------------------------------------------------------
EPOCH 45
---EPOCH 45: LOSS=1.43428
--------------------------------------------------------
EPOCH 46
---EPOCH 46: LOSS=1.42198
--------------------------------------------------------
EPOCH 47
---EPOCH 47: LOSS=1.41045
--------------------------------------------------------
EPOCH 48
---EPOCH 48: LOSS=1.39884
--------------------------------------------------------
EPOCH 49
---EPOCH 49: LOSS=1.38754
--------------------------------------------------------
EPOCH 50
---EPOCH 50: LOSS=1.37595
--------------------------------------------------------
EPOCH 51
---EPOCH 51: LOSS=1.36537
--------------------------------------------------------
EPOCH 52
---EPOCH 52: LOSS=1.35439
--------------------------------------------------------
EPOCH 53
---EPOCH 53: LOSS=1.34396
--------------------------------------------------------
EPOCH 54
---EPOCH 54: LOSS=1.33364
--------------------------------------------------------
EPOCH 55
---EPOCH 55: LOSS=1.32355
--------------------------------------------------------
EPOCH 56
---EPOCH 56: LOSS=1.31396
--------------------------------------------------------
EPOCH 57
---EPOCH 57: LOSS=1.30403
--------------------------------------------------------
EPOCH 58
---EPOCH 58: LOSS=1.29492
--------------------------------------------------------
EPOCH 59
---EPOCH 59: LOSS=1.28592
--------------------------------------------------------
EPOCH 60
---EPOCH 60: LOSS=1.27648
--------------------------------------------------------
EPOCH 61
---EPOCH 61: LOSS=1.26807
--------------------------------------------------------
EPOCH 62
---EPOCH 62: LOSS=1.25886
--------------------------------------------------------
EPOCH 63
---EPOCH 63: LOSS=1.24998
--------------------------------------------------------
EPOCH 64
---EPOCH 64: LOSS=1.24253
--------------------------------------------------------
EPOCH 65
---EPOCH 65: LOSS=1.23380
--------------------------------------------------------
EPOCH 66
---EPOCH 66: LOSS=1.22571
--------------------------------------------------------
EPOCH 67
---EPOCH 67: LOSS=1.21752
--------------------------------------------------------
EPOCH 68
---EPOCH 68: LOSS=1.20935
--------------------------------------------------------
EPOCH 69
---EPOCH 69: LOSS=1.20106
--------------------------------------------------------
EPOCH 70
---EPOCH 70: LOSS=1.19357
--------------------------------------------------------
EPOCH 71
---EPOCH 71: LOSS=1.18530
--------------------------------------------------------
EPOCH 72
---EPOCH 72: LOSS=1.17783
--------------------------------------------------------
EPOCH 73
---EPOCH 73: LOSS=1.17038
--------------------------------------------------------
EPOCH 74
---EPOCH 74: LOSS=1.16301
--------------------------------------------------------
EPOCH 75
---EPOCH 75: LOSS=1.15555
--------------------------------------------------------
EPOCH 76
---EPOCH 76: LOSS=1.14873
--------------------------------------------------------
EPOCH 77
---EPOCH 77: LOSS=1.14144
--------------------------------------------------------
EPOCH 78
---EPOCH 78: LOSS=1.13460
--------------------------------------------------------
EPOCH 79
---EPOCH 79: LOSS=1.12822
--------------------------------------------------------
EPOCH 80
---EPOCH 80: LOSS=1.12192
--------------------------------------------------------
EPOCH 81
---EPOCH 81: LOSS=1.11462
--------------------------------------------------------
EPOCH 82
---EPOCH 82: LOSS=1.10887
--------------------------------------------------------
EPOCH 83
---EPOCH 83: LOSS=1.10173
--------------------------------------------------------
EPOCH 84
---EPOCH 84: LOSS=1.09590
--------------------------------------------------------
EPOCH 85
---EPOCH 85: LOSS=1.08958
--------------------------------------------------------
EPOCH 86
---EPOCH 86: LOSS=1.08340
--------------------------------------------------------
EPOCH 87
---EPOCH 87: LOSS=1.07730
--------------------------------------------------------
EPOCH 88
---EPOCH 88: LOSS=1.07135
--------------------------------------------------------
EPOCH 89
---EPOCH 89: LOSS=1.06552
--------------------------------------------------------
EPOCH 90
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---EPOCH 90: LOSS=1.05997
--------------------------------------------------------
EPOCH 91
---EPOCH 91: LOSS=1.05459
--------------------------------------------------------
EPOCH 92
---EPOCH 92: LOSS=1.04855
--------------------------------------------------------
EPOCH 93
---EPOCH 93: LOSS=1.04280
--------------------------------------------------------
EPOCH 94
---EPOCH 94: LOSS=1.03749
--------------------------------------------------------
EPOCH 95
---EPOCH 95: LOSS=1.03215
--------------------------------------------------------
EPOCH 96
---EPOCH 96: LOSS=1.02657
--------------------------------------------------------
EPOCH 97
---EPOCH 97: LOSS=1.02164
--------------------------------------------------------
EPOCH 98
---EPOCH 98: LOSS=1.01595
--------------------------------------------------------
EPOCH 99
---EPOCH 99: LOSS=1.01088
--------------------------------------------------------
EPOCH 100
---EPOCH 100: LOSS=1.00570
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_30_3.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_30_3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training and testing accuracies</span>
<span class="n">Exp1_train_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span> <span class="n">Exp1_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1_W2_trained</span><span class="p">,</span> 
                                       <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                       <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                       <span class="n">test_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Generate the test data</span>
<span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="o">*</span><span class="n">N_TESTING_BATCHES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp1_test_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span><span class="p">,</span> <span class="n">Exp1_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp1_W2_trained</span><span class="p">,</span> 
                                      <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                      <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                      <span class="n">test_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train classifier accuracy: 77.6%
Train absolute error: 5.2 deg

Test classifier accuracy: 53.9%
Test absolute error: 8.7 deg
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_31_1.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_31_1.png" />
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_31_2.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_31_2.png" />
</div>
</div>
<ul class="simple">
<li><p>The train and test accuracies are similar to the accuracies in the Starting Notebook, where train accuracy = 82.9% (4.7 degree absolute error) and test accuracy = 54.8% (7.9 degree absolute error). Note: the core differences between the notebooks in terms of hyperparameters is the learning rate (from 0.01 to 0.0001) and number of epochs (from 10 to 100).</p></li>
<li><p>The confusion matrices look close to the confusion matrices in the Starting Notebook too, and are looking as expected (i.e., the network is making errors that are close to the true IPD, so there aren’t vast differences between the estimated IPD and the true IPD). We might note something peculiar for true IPD &gt; 50 in the confusion matrix on test data: It looks like for true IPDs that are greater than 50, the network is biased to output a higher estimate around 80 (why?).</p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="exp-2-training-testing-results">
<h2>Exp 2. Training &amp; Testing Results<a class="headerlink" href="#exp-2-training-testing-results" title="Permalink to this headline">¶</a></h2>
<div class="section" id="using-the-usual-phase-delays">
<h3>Using the usual phase delays<a class="headerlink" href="#using-the-usual-phase-delays" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training</span>
<span class="c1"># Generate the training data</span>
<span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">NUM_SAMPLES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp2A_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2A_W2_trained</span><span class="p">,</span> <span class="n">snn_training_snapshot</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span> 
                                                                          <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                          <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                                                          <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "1a3ff6bd8bcc452a9aad92fbcafbb655", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------
EPOCH 1
---EPOCH 1: LOSS=3.40958
--------------------------------------------------------
EPOCH 2
---EPOCH 2: LOSS=2.72205
--------------------------------------------------------
EPOCH 3
---EPOCH 3: LOSS=2.51187
--------------------------------------------------------
EPOCH 4
---EPOCH 4: LOSS=2.44705
--------------------------------------------------------
EPOCH 5
---EPOCH 5: LOSS=2.40847
--------------------------------------------------------
EPOCH 6
---EPOCH 6: LOSS=2.37829
--------------------------------------------------------
EPOCH 7
---EPOCH 7: LOSS=2.35708
--------------------------------------------------------
EPOCH 8
---EPOCH 8: LOSS=2.33536
--------------------------------------------------------
EPOCH 9
---EPOCH 9: LOSS=2.31048
--------------------------------------------------------
EPOCH 10
---EPOCH 10: LOSS=2.28252
--------------------------------------------------------
EPOCH 11
---EPOCH 11: LOSS=2.26207
--------------------------------------------------------
EPOCH 12
---EPOCH 12: LOSS=2.24304
--------------------------------------------------------
EPOCH 13
---EPOCH 13: LOSS=2.22472
--------------------------------------------------------
EPOCH 14
---EPOCH 14: LOSS=2.20403
--------------------------------------------------------
EPOCH 15
---EPOCH 15: LOSS=2.18575
--------------------------------------------------------
EPOCH 16
---EPOCH 16: LOSS=2.17183
--------------------------------------------------------
EPOCH 17
---EPOCH 17: LOSS=2.15362
--------------------------------------------------------
EPOCH 18
---EPOCH 18: LOSS=2.13579
--------------------------------------------------------
EPOCH 19
---EPOCH 19: LOSS=2.12290
--------------------------------------------------------
EPOCH 20
---EPOCH 20: LOSS=2.10203
--------------------------------------------------------
EPOCH 21
---EPOCH 21: LOSS=2.09150
--------------------------------------------------------
EPOCH 22
---EPOCH 22: LOSS=2.06634
--------------------------------------------------------
EPOCH 23
---EPOCH 23: LOSS=2.05068
--------------------------------------------------------
EPOCH 24
---EPOCH 24: LOSS=2.03653
--------------------------------------------------------
EPOCH 25
---EPOCH 25: LOSS=2.02003
--------------------------------------------------------
EPOCH 26
---EPOCH 26: LOSS=1.99771
--------------------------------------------------------
EPOCH 27
---EPOCH 27: LOSS=1.97866
--------------------------------------------------------
EPOCH 28
---EPOCH 28: LOSS=1.95651
--------------------------------------------------------
EPOCH 29
---EPOCH 29: LOSS=1.93429
--------------------------------------------------------
EPOCH 30
---EPOCH 30: LOSS=1.91328
--------------------------------------------------------
EPOCH 31
---EPOCH 31: LOSS=1.90007
--------------------------------------------------------
EPOCH 32
---EPOCH 32: LOSS=1.87894
--------------------------------------------------------
EPOCH 33
---EPOCH 33: LOSS=1.85858
--------------------------------------------------------
EPOCH 34
---EPOCH 34: LOSS=1.84496
--------------------------------------------------------
EPOCH 35
---EPOCH 35: LOSS=1.82192
--------------------------------------------------------
EPOCH 36
---EPOCH 36: LOSS=1.81130
--------------------------------------------------------
EPOCH 37
---EPOCH 37: LOSS=1.79357
--------------------------------------------------------
EPOCH 38
---EPOCH 38: LOSS=1.77924
--------------------------------------------------------
EPOCH 39
---EPOCH 39: LOSS=1.76874
--------------------------------------------------------
EPOCH 40
---EPOCH 40: LOSS=1.75147
--------------------------------------------------------
EPOCH 41
---EPOCH 41: LOSS=1.73578
--------------------------------------------------------
EPOCH 42
---EPOCH 42: LOSS=1.72591
--------------------------------------------------------
EPOCH 43
---EPOCH 43: LOSS=1.71363
--------------------------------------------------------
EPOCH 44
---EPOCH 44: LOSS=1.69534
--------------------------------------------------------
EPOCH 45
---EPOCH 45: LOSS=1.69050
--------------------------------------------------------
EPOCH 46
---EPOCH 46: LOSS=1.67668
--------------------------------------------------------
EPOCH 47
---EPOCH 47: LOSS=1.66817
--------------------------------------------------------
EPOCH 48
---EPOCH 48: LOSS=1.65287
--------------------------------------------------------
EPOCH 49
---EPOCH 49: LOSS=1.63998
--------------------------------------------------------
EPOCH 50
---EPOCH 50: LOSS=1.62602
--------------------------------------------------------
EPOCH 51
---EPOCH 51: LOSS=1.61637
--------------------------------------------------------
EPOCH 52
---EPOCH 52: LOSS=1.60314
--------------------------------------------------------
EPOCH 53
---EPOCH 53: LOSS=1.59116
--------------------------------------------------------
EPOCH 54
---EPOCH 54: LOSS=1.57840
--------------------------------------------------------
EPOCH 55
---EPOCH 55: LOSS=1.57105
--------------------------------------------------------
EPOCH 56
---EPOCH 56: LOSS=1.55589
--------------------------------------------------------
EPOCH 57
---EPOCH 57: LOSS=1.54777
--------------------------------------------------------
EPOCH 58
---EPOCH 58: LOSS=1.53843
--------------------------------------------------------
EPOCH 59
---EPOCH 59: LOSS=1.52523
--------------------------------------------------------
EPOCH 60
---EPOCH 60: LOSS=1.52074
--------------------------------------------------------
EPOCH 61
---EPOCH 61: LOSS=1.50545
--------------------------------------------------------
EPOCH 62
---EPOCH 62: LOSS=1.49435
--------------------------------------------------------
EPOCH 63
---EPOCH 63: LOSS=1.49094
--------------------------------------------------------
EPOCH 64
---EPOCH 64: LOSS=1.47730
--------------------------------------------------------
EPOCH 65
---EPOCH 65: LOSS=1.47072
--------------------------------------------------------
EPOCH 66
---EPOCH 66: LOSS=1.46408
--------------------------------------------------------
EPOCH 67
---EPOCH 67: LOSS=1.45343
--------------------------------------------------------
EPOCH 68
---EPOCH 68: LOSS=1.44766
--------------------------------------------------------
EPOCH 69
---EPOCH 69: LOSS=1.43703
--------------------------------------------------------
EPOCH 70
---EPOCH 70: LOSS=1.43032
--------------------------------------------------------
EPOCH 71
---EPOCH 71: LOSS=1.41941
--------------------------------------------------------
EPOCH 72
---EPOCH 72: LOSS=1.41668
--------------------------------------------------------
EPOCH 73
---EPOCH 73: LOSS=1.40902
--------------------------------------------------------
EPOCH 74
---EPOCH 74: LOSS=1.39810
--------------------------------------------------------
EPOCH 75
---EPOCH 75: LOSS=1.38761
--------------------------------------------------------
EPOCH 76
---EPOCH 76: LOSS=1.38506
--------------------------------------------------------
EPOCH 77
---EPOCH 77: LOSS=1.37743
--------------------------------------------------------
EPOCH 78
---EPOCH 78: LOSS=1.37311
--------------------------------------------------------
EPOCH 79
---EPOCH 79: LOSS=1.36532
--------------------------------------------------------
EPOCH 80
---EPOCH 80: LOSS=1.35862
--------------------------------------------------------
EPOCH 81
---EPOCH 81: LOSS=1.35002
--------------------------------------------------------
EPOCH 82
---EPOCH 82: LOSS=1.34871
--------------------------------------------------------
EPOCH 83
---EPOCH 83: LOSS=1.33580
--------------------------------------------------------
EPOCH 84
---EPOCH 84: LOSS=1.33071
--------------------------------------------------------
EPOCH 85
---EPOCH 85: LOSS=1.32521
--------------------------------------------------------
EPOCH 86
---EPOCH 86: LOSS=1.31753
--------------------------------------------------------
EPOCH 87
---EPOCH 87: LOSS=1.31447
--------------------------------------------------------
EPOCH 88
---EPOCH 88: LOSS=1.30784
--------------------------------------------------------
EPOCH 89
---EPOCH 89: LOSS=1.29905
--------------------------------------------------------
EPOCH 90
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---EPOCH 90: LOSS=1.29086
--------------------------------------------------------
EPOCH 91
---EPOCH 91: LOSS=1.28954
--------------------------------------------------------
EPOCH 92
---EPOCH 92: LOSS=1.28230
--------------------------------------------------------
EPOCH 93
---EPOCH 93: LOSS=1.27364
--------------------------------------------------------
EPOCH 94
---EPOCH 94: LOSS=1.27162
--------------------------------------------------------
EPOCH 95
---EPOCH 95: LOSS=1.26588
--------------------------------------------------------
EPOCH 96
---EPOCH 96: LOSS=1.26003
--------------------------------------------------------
EPOCH 97
---EPOCH 97: LOSS=1.25150
--------------------------------------------------------
EPOCH 98
---EPOCH 98: LOSS=1.24451
--------------------------------------------------------
EPOCH 99
---EPOCH 99: LOSS=1.24208
--------------------------------------------------------
EPOCH 100
---EPOCH 100: LOSS=1.23542
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_36_3.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_36_3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training and testing accuracies</span>
<span class="n">Exp2A_train_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span> <span class="n">Exp2A_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2A_W2_trained</span><span class="p">,</span> 
                                        <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                        <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                        <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                        <span class="n">test_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Generate the test data</span>
<span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="o">*</span><span class="n">N_TESTING_BATCHES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Exp2A_test_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span><span class="p">,</span> <span class="n">Exp2A_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2A_W2_trained</span><span class="p">,</span> 
                                       <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                       <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                       <span class="n">test_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train classifier accuracy: 61.6%
Train absolute error: 7.6 deg

Test classifier accuracy: 29.2%
Test absolute error: 28.6 deg
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_37_1.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_37_1.png" />
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_37_2.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_37_2.png" />
</div>
</div>
<ul class="simple">
<li><p>While the confusion matrix for the training data looks relatively normal, the confusion matrix of the test data seems to suggest this:</p>
<ul>
<li><p>for negative true IPDs, the network is biased to output lower estimated IPDs or much higher, positive IPDs; and it seems like only some values get outputted (-20, -50, -80)</p></li>
<li><p>for positive true IPDs, the network seems to perform a bit better, but there seems to be a range of true IPDs from 0 to 50, where the network is biased to output an estimate between 0 and 20.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="using-no-phase-delays">
<h3>Using no phase delays<a class="headerlink" href="#using-no-phase-delays" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training</span>
<span class="c1"># Generate the training data</span>
<span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">NUM_SAMPLES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Exp2B_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2B_W2_trained</span><span class="p">,</span> <span class="n">snn_training_snapshot</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span>
                                                                          <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                                          <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                                                          <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "31205617a67c4e2990c7e83e7ca5eaaa", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--------------------------------------------------------
EPOCH 1
---EPOCH 1: LOSS=2.95107
--------------------------------------------------------
EPOCH 2
---EPOCH 2: LOSS=2.54748
--------------------------------------------------------
EPOCH 3
---EPOCH 3: LOSS=2.47353
--------------------------------------------------------
EPOCH 4
---EPOCH 4: LOSS=2.44364
--------------------------------------------------------
EPOCH 5
---EPOCH 5: LOSS=2.42513
--------------------------------------------------------
EPOCH 6
---EPOCH 6: LOSS=2.40976
--------------------------------------------------------
EPOCH 7
---EPOCH 7: LOSS=2.39648
--------------------------------------------------------
EPOCH 8
---EPOCH 8: LOSS=2.38305
--------------------------------------------------------
EPOCH 9
---EPOCH 9: LOSS=2.36896
--------------------------------------------------------
EPOCH 10
---EPOCH 10: LOSS=2.35604
--------------------------------------------------------
EPOCH 11
---EPOCH 11: LOSS=2.33839
--------------------------------------------------------
EPOCH 12
---EPOCH 12: LOSS=2.32547
--------------------------------------------------------
EPOCH 13
---EPOCH 13: LOSS=2.30925
--------------------------------------------------------
EPOCH 14
---EPOCH 14: LOSS=2.29438
--------------------------------------------------------
EPOCH 15
---EPOCH 15: LOSS=2.28152
--------------------------------------------------------
EPOCH 16
---EPOCH 16: LOSS=2.27098
--------------------------------------------------------
EPOCH 17
---EPOCH 17: LOSS=2.25595
--------------------------------------------------------
EPOCH 18
---EPOCH 18: LOSS=2.24193
--------------------------------------------------------
EPOCH 19
---EPOCH 19: LOSS=2.22514
--------------------------------------------------------
EPOCH 20
---EPOCH 20: LOSS=2.21801
--------------------------------------------------------
EPOCH 21
---EPOCH 21: LOSS=2.20560
--------------------------------------------------------
EPOCH 22
---EPOCH 22: LOSS=2.19210
--------------------------------------------------------
EPOCH 23
---EPOCH 23: LOSS=2.18075
--------------------------------------------------------
EPOCH 24
---EPOCH 24: LOSS=2.16932
--------------------------------------------------------
EPOCH 25
---EPOCH 25: LOSS=2.15740
--------------------------------------------------------
EPOCH 26
---EPOCH 26: LOSS=2.14589
--------------------------------------------------------
EPOCH 27
---EPOCH 27: LOSS=2.13579
--------------------------------------------------------
EPOCH 28
---EPOCH 28: LOSS=2.12147
--------------------------------------------------------
EPOCH 29
---EPOCH 29: LOSS=2.11487
--------------------------------------------------------
EPOCH 30
---EPOCH 30: LOSS=2.10265
--------------------------------------------------------
EPOCH 31
---EPOCH 31: LOSS=2.09105
--------------------------------------------------------
EPOCH 32
---EPOCH 32: LOSS=2.08155
--------------------------------------------------------
EPOCH 33
---EPOCH 33: LOSS=2.06933
--------------------------------------------------------
EPOCH 34
---EPOCH 34: LOSS=2.06054
--------------------------------------------------------
EPOCH 35
---EPOCH 35: LOSS=2.04819
--------------------------------------------------------
EPOCH 36
---EPOCH 36: LOSS=2.04145
--------------------------------------------------------
EPOCH 37
---EPOCH 37: LOSS=2.03199
--------------------------------------------------------
EPOCH 38
---EPOCH 38: LOSS=2.02220
--------------------------------------------------------
EPOCH 39
---EPOCH 39: LOSS=2.00925
--------------------------------------------------------
EPOCH 40
---EPOCH 40: LOSS=2.00127
--------------------------------------------------------
EPOCH 41
---EPOCH 41: LOSS=1.99124
--------------------------------------------------------
EPOCH 42
---EPOCH 42: LOSS=1.98126
--------------------------------------------------------
EPOCH 43
---EPOCH 43: LOSS=1.97069
--------------------------------------------------------
EPOCH 44
---EPOCH 44: LOSS=1.96168
--------------------------------------------------------
EPOCH 45
---EPOCH 45: LOSS=1.95291
--------------------------------------------------------
EPOCH 46
---EPOCH 46: LOSS=1.94480
--------------------------------------------------------
EPOCH 47
---EPOCH 47: LOSS=1.93516
--------------------------------------------------------
EPOCH 48
---EPOCH 48: LOSS=1.92807
--------------------------------------------------------
EPOCH 49
---EPOCH 49: LOSS=1.91699
--------------------------------------------------------
EPOCH 50
---EPOCH 50: LOSS=1.90768
--------------------------------------------------------
EPOCH 51
---EPOCH 51: LOSS=1.89594
--------------------------------------------------------
EPOCH 52
---EPOCH 52: LOSS=1.88880
--------------------------------------------------------
EPOCH 53
---EPOCH 53: LOSS=1.87916
--------------------------------------------------------
EPOCH 54
---EPOCH 54: LOSS=1.87416
--------------------------------------------------------
EPOCH 55
---EPOCH 55: LOSS=1.86425
--------------------------------------------------------
EPOCH 56
---EPOCH 56: LOSS=1.85387
--------------------------------------------------------
EPOCH 57
---EPOCH 57: LOSS=1.84551
--------------------------------------------------------
EPOCH 58
---EPOCH 58: LOSS=1.83786
--------------------------------------------------------
EPOCH 59
---EPOCH 59: LOSS=1.82848
--------------------------------------------------------
EPOCH 60
---EPOCH 60: LOSS=1.82234
--------------------------------------------------------
EPOCH 61
---EPOCH 61: LOSS=1.81547
--------------------------------------------------------
EPOCH 62
---EPOCH 62: LOSS=1.80029
--------------------------------------------------------
EPOCH 63
---EPOCH 63: LOSS=1.79828
--------------------------------------------------------
EPOCH 64
---EPOCH 64: LOSS=1.78981
--------------------------------------------------------
EPOCH 65
---EPOCH 65: LOSS=1.78739
--------------------------------------------------------
EPOCH 66
---EPOCH 66: LOSS=1.77644
--------------------------------------------------------
EPOCH 67
---EPOCH 67: LOSS=1.76860
--------------------------------------------------------
EPOCH 68
---EPOCH 68: LOSS=1.76367
--------------------------------------------------------
EPOCH 69
---EPOCH 69: LOSS=1.75468
--------------------------------------------------------
EPOCH 70
---EPOCH 70: LOSS=1.74542
--------------------------------------------------------
EPOCH 71
---EPOCH 71: LOSS=1.74373
--------------------------------------------------------
EPOCH 72
---EPOCH 72: LOSS=1.73410
--------------------------------------------------------
EPOCH 73
---EPOCH 73: LOSS=1.72939
--------------------------------------------------------
EPOCH 74
---EPOCH 74: LOSS=1.72096
--------------------------------------------------------
EPOCH 75
---EPOCH 75: LOSS=1.71750
--------------------------------------------------------
EPOCH 76
---EPOCH 76: LOSS=1.70512
--------------------------------------------------------
EPOCH 77
---EPOCH 77: LOSS=1.70103
--------------------------------------------------------
EPOCH 78
---EPOCH 78: LOSS=1.69529
--------------------------------------------------------
EPOCH 79
---EPOCH 79: LOSS=1.69222
--------------------------------------------------------
EPOCH 80
---EPOCH 80: LOSS=1.68562
--------------------------------------------------------
EPOCH 81
---EPOCH 81: LOSS=1.67820
--------------------------------------------------------
EPOCH 82
---EPOCH 82: LOSS=1.67356
--------------------------------------------------------
EPOCH 83
---EPOCH 83: LOSS=1.65878
--------------------------------------------------------
EPOCH 84
---EPOCH 84: LOSS=1.65979
--------------------------------------------------------
EPOCH 85
---EPOCH 85: LOSS=1.65250
--------------------------------------------------------
EPOCH 86
---EPOCH 86: LOSS=1.64793
--------------------------------------------------------
EPOCH 87
---EPOCH 87: LOSS=1.64280
--------------------------------------------------------
EPOCH 88
---EPOCH 88: LOSS=1.63502
--------------------------------------------------------
EPOCH 89
---EPOCH 89: LOSS=1.62906
--------------------------------------------------------
EPOCH 90
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---EPOCH 90: LOSS=1.62396
--------------------------------------------------------
EPOCH 91
---EPOCH 91: LOSS=1.61994
--------------------------------------------------------
EPOCH 92
---EPOCH 92: LOSS=1.61217
--------------------------------------------------------
EPOCH 93
---EPOCH 93: LOSS=1.60495
--------------------------------------------------------
EPOCH 94
---EPOCH 94: LOSS=1.60373
--------------------------------------------------------
EPOCH 95
---EPOCH 95: LOSS=1.59979
--------------------------------------------------------
EPOCH 96
---EPOCH 96: LOSS=1.59587
--------------------------------------------------------
EPOCH 97
---EPOCH 97: LOSS=1.58952
--------------------------------------------------------
EPOCH 98
---EPOCH 98: LOSS=1.58326
--------------------------------------------------------
EPOCH 99
---EPOCH 99: LOSS=1.58086
--------------------------------------------------------
EPOCH 100
---EPOCH 100: LOSS=1.57417
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_40_3.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_40_3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># training and testing accuracies</span>
<span class="n">Exp2B_train_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_train</span><span class="p">,</span> <span class="n">spikes_train</span><span class="p">,</span> <span class="n">Exp2B_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2B_W2_trained</span><span class="p">,</span> 
                                        <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                        <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                        <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                        <span class="n">test_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Generate the test data</span>
<span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span> <span class="o">=</span> <span class="n">random_ipd_input_signal</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="o">*</span><span class="n">N_TESTING_BATCHES</span><span class="p">,</span> <span class="n">usual_phase_delays</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Exp2B_test_accuracy</span> <span class="o">=</span> <span class="n">analyse_accuracy</span><span class="p">(</span><span class="n">ipds_test</span><span class="p">,</span> <span class="n">spikes_test</span><span class="p">,</span> <span class="n">Exp2B_W1_trained</span><span class="p">,</span> <span class="n">W1_bis</span><span class="p">,</span> <span class="n">Exp2B_W2_trained</span><span class="p">,</span> 
                                       <span class="n">multiple_connections_per_axon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">random_tau_constants</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                                       <span class="n">minimal_smoothing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                                       <span class="n">test_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train classifier accuracy: 51.9%
Train absolute error: 11.3 deg

Test classifier accuracy: 43.8%
Test absolute error: 15.6 deg
</pre></div>
</div>
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_41_1.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_41_1.png" />
<img alt="../_images/Filter-and-Fire_Neuron_Model_SNN_41_2.png" src="../_images/Filter-and-Fire_Neuron_Model_SNN_41_2.png" />
</div>
</div>
<ul class="simple">
<li><p>Compared to Exp 2.a, the test accuracy here is larger (from 29.2% to 43.8%), suggesting that we might not need phase delays (i.e., set to 0).</p></li>
<li><p>We could try to train the network with null phase delays (like Exp 2.b here) and fixed <code class="docutils literal notranslate"><span class="pre">tau_rise</span></code> and <code class="docutils literal notranslate"><span class="pre">tau_decay</span></code> hyperparameters (like Exp 1 normal smoothing) to see if these two features work well together for the training on the sound localisation task.</p></li>
</ul>
</div>
</div>
<hr class="docutils" />
<div class="section" id="exp-3-training-testing-results">
<h2>Exp 3. Training &amp; Testing Results<a class="headerlink" href="#exp-3-training-testing-results" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./research"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Learning_delays.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">&lt;no title&gt;</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Excitatory-only-localisation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sound localisation with excitatory-only inputs surrogate gradient descent</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By COMOB, the project for collaborative modelling of the brain<br/>
    
      <div class="extra_footer">
        <small>
  Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA</a>:
  You may use this work, with attribution, in other freely available works.
</small>

      </div>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>